<!DOCTYPE html>
<html>
   <head>
      <title>Multi-Label Topic Classification on Reuters Newswire Articles</title>
      <link rel="stylesheet" href="./front_end/index.css">
   </head>
   <body>
      <div id="introduction">
         <center>
            <h1>Multi-Label Topic Classification on Reuters Newswire Articles</h1>
         </center>
         <p class="common-body-text">
            This document covers deep-learning methods investigated to perform multi-label text classification on the <a href="http://kdd.ics.uci.edu/databases/reuters21578/reuters21578.html">Reuters-21578 Text Categorization Collection</a>.
         </p>
         <table id="top-page-splitting-table">
           <tr>
             <td>
	       <p class="common-body-text">Their architectures and performances are shown below:
		 <center id="top-level-stats">
		   <table>
		     <tr>
		       <th>Architecture</th>
		       <th>F1 Score</th>
		     </tr>
		     <tr>
		       <td>CNN</td>
		       <td>0.7655879</td>
		     </tr>
		     <tr>
		       <td>Dense Neural Network</td>
		       <td>0.6601627</td>
		     </tr>
		     <tr>
		       <td>RNN (LSTM+Attention)</td>
		       <td>0.1065762</td>
		     </tr>
		   </table>
	       </p>
	       <p class="common-body-text">
		 The code used in this document can be found <a href="https://github.com/paul-tqh-nguyen/reuters_topic_labelling">here</a>.
	       </p>
	       <p class="common-body-text">
		 Table of Contents:
		 <ul>
		   <li><a href="#problem-description">Data Description & Problem Overview</a></li>
		   <li><a href="#methods-employed">Methods Employed & Performance Evaluation</a></li>
		   <li><a href="#implementation-details">Implementation Details & Code Demonstration</a></li>
		   <li><a href="#conclusion">Conclusion & Lessons Learned</a></li>
		 </ul>
	       </p>
	     </td>
	     <td>
                  <table>
                     <tr>
                       <td style="text-align: left">
			 <center>
                          <pre style="scroll-direction: horizontal;">
			    <code style ="width: 30vw; overflow: scroll; color: #eee; background-color: rgb(48, 10, 36, 1.0); text-align: left; padding-left: 5%; padding-right: 5%; padding-top: 0%; padding-bottom: 0%;">
The Panamanian bulk carrier Juvena is
still aground outside Tartous, Syria, despite discharging 6,400
tons of its 39,000-ton cargo of wheat, and water has entered
the engine-room due to a crack in the vessel bottom, Lloyds
Shipping Intelligence Service said.
    The Juvena, 53,351 tonnes dw, ran aground outside Tartous
port basin breakwater on February 25 in heavy weather and rough
seas.
 Reuter
			    </code>
			  </pre>
			  </center>
                        </td>
                     </tr>
                     <tr style="position: relative;">
                        <td style="text-align: center; font-size: 10vw; color: #eee; margin: 0; height: 5vw;  -ms-transform: translateY(-10%); transform: translateY(-10%);">&#8650;&#8650;</td>
                     </tr>
                     <tr>
                       <td style="text-align: left">
			 <center>
                           <pre>
			     <code style ="color: #eee; background-color: rgb(48, 10, 36, 1.0); width: 50%; padding-left: 5%; padding-right: 5%; padding-top: 0%; padding-bottom: 0%;">
     acq <strong style="color: grey;">False</strong>
    corn <strong style="color: grey;">False</strong>
   crude <strong style="color: grey;">False</strong>
    earn <strong style="color: grey;">False</strong>
   grain <strong style="color: #df00fe;">True</strong>
interest <strong style="color: grey;">False</strong>
money-fx <strong style="color: grey;">False</strong>
    ship <strong style="color: #df00fe;">True</strong>
   trade <strong style="color: grey;">False</strong>
   wheat <strong style="color: #df00fe;">True</strong>
			     </code>
			   </pre>
			 </center>
                       </td>
                     </tr>
		  </table>
	      </td>
            </tr>
	 </table>
      </div>
      <main>
         <div>
            <div id="top-spacer"></div>
            </br>
            <script>if (["#problem-description","#methods-employed","#implementation-details","#conclusion"].includes(window.location.hash)) {document.getElementById("top-spacer").style.height = "20vh";}</script>
            <div class="topic-section">
               <center id="problem-description">
                  <h2 class="common-UppercaseTitle">
                     <svg class="section-icon">
                        <circle fill="#eeb5f7" cx="33" cy="33" r="33">
                        </circle>
                        <path d="M15.7 45.3c-.7-2-.7-3.3-.7-8v-8.7c0-4.6 0-6 .7-8 .8-2.2 2.7-4 5-5 2-.6 3.3-.6 8-.6h8.7c4.6 0 6 0 8 .7 2.2.8 4 2.7 5 5 .6 2 .6 3.3.6 8v8.7c0 4.6 0 6-.7 8-.8 2.2-2.7 4-5 5-2 .6-3.3.6-8 .6h-8.7c-4.6 0-6 0-8-.7-2.2-.8-4-2.7-5-5z" fill="#beb5eb"></path>
                        <g>
                           <rect fill="#8e80ff" x="23" y="27" width="20" height="2" rx="1"></rect>
                           <circle fill="#8e80ff" cx="27.000185427800893" cy="28" r="4"></circle>
                        </g>
                        <g>
                           <rect fill="#8e80ff" x="23" y="37" width="20" height="2" rx="1"></rect>
                           <circle fill="#8e80ff" cx="27.0000000285928" cy="38" r="4"></circle>
                        </g>
                     </svg>
                     <span>
                     Data Description & Problem Overview
                     </span>
                  </h2>
               </center>
               <h3>Original Dataset Overview</h3>
               <p class="common-body-text">
                  The dataset being explored is <a href="http://kdd.ics.uci.edu/databases/reuters21578/reuters21578.html">Reuters-21578 Text Categorization Collection</a>.
               </p>
               <p class="common-body-text">
                  A copy of the data that can be found in the <a href="https://github.com/paul-tqh-nguyen/reuters_topic_labelling/tree/master/data">repository</a>.
               </p>
               <p class="common-body-text">
                  The dataset's provided <a href="https://github.com/paul-tqh-nguyen/reuters_topic_labelling/blob/master/data/README.txt">README</a> gives a good summary of what is provided. I'll provide a brief recap of the most pertinent content here:
               <ul>
                  <li>The data is provided in <a href="https://en.wikipedia.org/wiki/Standard_Generalized_Markup_Language">SGML format</a>.</li>
                  <li>The data contains 21,578 Reuters Newswire articles from 1987.</li>
                  <li>There are 21 <code>.sgm</code> files with 1000 elements each. The 22nd and last <code>.sgm</code> file contains 578 elements. I'll examine those elements later in this section.</li>
                  <li>They also provide a list of exchanges, orgs, people, places, and topics. All of this can be extracted directly form the <code>.sgm</code> files.</li>
               </ul>
               </p>
               <p class="common-body-text">
                  Here's a brief look at some of the <code>.sgm</code> content:
               <pre>
		<code>
&lt;!DOCTYPE lewis SYSTEM &quot;lewis.dtd&quot;&gt;
&lt;REUTERS TOPICS=&quot;NO&quot; LEWISSPLIT=&quot;TEST&quot; CGISPLIT=&quot;TRAINING-SET&quot; OLDID=&quot;20436&quot; NEWID=&quot;21001&quot;&gt;
&lt;DATE&gt;19-OCT-1987 15:37:46.03&lt;/DATE&gt;
&lt;TOPICS&gt;&lt;/TOPICS&gt;
&lt;PLACES&gt;&lt;/PLACES&gt;
&lt;PEOPLE&gt;&lt;/PEOPLE&gt;
&lt;ORGS&gt;&lt;/ORGS&gt;
&lt;EXCHANGES&gt;&lt;/EXCHANGES&gt;
&lt;COMPANIES&gt;&lt;/COMPANIES&gt;
&lt;UNKNOWN&gt; 
&amp;#5;&amp;#5;&amp;#5;F 
&amp;#22;&amp;#22;&amp;#1;f2882&amp;#31;reute
f f BC-CITYFED-FINANCI   10-19 0013&lt;/UNKNOWN&gt;
&lt;TEXT TYPE=&quot;BRIEF&quot;&gt;&amp;#2;
******&lt;TITLE&gt;CITYFED FINANCIAL CORP SAYS IT CUT QTRLY DIVIDEND TO ONE CENT FROM 10 CTS/SHR
&lt;/TITLE&gt;Blah blah blah.
&amp;#3;

&lt;/TEXT&gt;
&lt;/REUTERS&gt;
&lt;REUTERS TOPICS=&quot;YES&quot; LEWISSPLIT=&quot;TEST&quot; CGISPLIT=&quot;TRAINING-SET&quot; OLDID=&quot;20435&quot; NEWID=&quot;21002&quot;&gt;
&lt;DATE&gt;19-OCT-1987 15:35:53.55&lt;/DATE&gt;
&lt;TOPICS&gt;&lt;D&gt;crude&lt;/D&gt;&lt;D&gt;ship&lt;/D&gt;&lt;/TOPICS&gt;
&lt;PLACES&gt;&lt;D&gt;bahrain&lt;/D&gt;&lt;D&gt;iran&lt;/D&gt;&lt;D&gt;usa&lt;/D&gt;&lt;/PLACES&gt;
&lt;PEOPLE&gt;&lt;/PEOPLE&gt;
&lt;ORGS&gt;&lt;/ORGS&gt;
&lt;EXCHANGES&gt;&lt;/EXCHANGES&gt;
&lt;COMPANIES&gt;&lt;/COMPANIES&gt;
&lt;UNKNOWN&gt; 
&amp;#5;&amp;#5;&amp;#5;Y 
		</code>
	      </pre>
               </p>
               <p class="common-body-text">
                  Note the presence of <code>"Blah blah blah."</code> in the <code>.sgm</code> file. This is discussed in the <a href="https://github.com/paul-tqh-nguyen/reuters_topic_labelling/blob/master/data/README.txt">README</a>. These strings are placed where there was missing content. This type of content among others are handled during data pre-processing.
               </p>
               <p class="common-body-text">
                  Each of the articles contained in this dataset were encapsulated in the <code>.sgm</code> files as a <code>&lt;REUTERS&gt;</code> element.
               </p>
               <p class="common-body-text">
                  Each <code>&lt;REUTERS&gt;</code> element could contain the following elements:
               <ul>
                  <li style="font-size: 20px; margin-bottom: 5px;"><code>&lt;DATE&gt;</code></li>
                  <li style="font-size: 20px; margin-bottom: 5px;"><code>&lt;TOPICS&gt;</code></li>
                  <li style="font-size: 20px; margin-bottom: 5px;"><code>&lt;PLACES&gt;</code></li>
                  <li style="font-size: 20px; margin-bottom: 5px;"><code>&lt;PEOPLE&gt;</code></li>
                  <li style="font-size: 20px; margin-bottom: 5px;"><code>&lt;ORGS&gt;</code></li>
                  <li style="font-size: 20px; margin-bottom: 5px;"><code>&lt;COMPANIES&gt;</code></li>
                  <li style="font-size: 20px; margin-bottom: 5px;"><code>&lt;TITLE&gt;</code></li>
                  <li style="font-size: 20px; margin-bottom: 5px;"><code>&lt;DATELINE&gt;</code></li>
                  <li style="font-size: 20px; margin-bottom: 5px;"><code>&lt;TEXT&gt;</code></li>
                  <li style="font-size: 20px; margin-bottom: 5px;"><code>&lt;UNKNOWN&gt;</code></li>
               </ul>
               </p>
               <p class="common-body-text">
                  Not all of the <code>&lt;REUTERS&gt;</code> elements contained elements for each of the tags described above. There was sometimes not present data (e.g. some articles didn't mention any significant individual people, so there was nothing listed in the <code>&lt;PEOPLE&gt;</code> tag or the <code>&lt;PEOPLE&gt;</code> was entirely not present) and missing data (very often noted by the <code>"Blah blah blah."</code> string).
               </p>
               <h3>Dataset Pre-Processing Broad Overview</h3>
               <center id="preprocessing-interface-table">
                  <table style="margin-left: 0">
                     <tr>
                       <td style="text-align: left">
			 <pre>
			   <code>
West German wholesale turnover
provisionally totalled about 58 billion marks in February, a
rise of a real one pct compared with the same month last year,
the Federal Statistics Office said.
			   </code>
			 </pre>
                       </td>
                     </tr>
                     <tr>
                       <td style="text-align: center; font-size: 5vw; color: darkviolet; margin: 0; height: 5vw;  -ms-transform: translateY(-10%); transform: translateY(-10%);">&#8650;&#8650;</td>
                     </tr>
                     <tr>
                       <td style="text-align: left">
			 <pre>
			   <code>
west german wholesale turnover provisionally total NUMBER billion mark february, rise real one pct compare month last year, federal statistic office said. 
			   </code>
			 </pre>
                       </td>
                     </tr>
		  </table>
	       </center>
               <p class="common-body-text">
                  This section will broadly go over the pre-processing pipeline.
               </p>
               <p class="common-body-text">
                  I used <a href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/">Beautiful Soup</a> to parse the <code>.sgm</code> files.
               </p>
               <p class="common-body-text">
                  The pre-processing pipeline generates two files, <a href="https://github.com/paul-tqh-nguyen/reuters_topic_labelling/blob/master/preprocessed_data/all_extracted_data.csv"><code>all_extracted_data.csv</code></a> and <a href="https://github.com/paul-tqh-nguyen/reuters_topic_labelling/blob/master/preprocessed_data/topics_data.csv"><code>topics_data.csv</code></a>.
               </p>
               <p class="common-body-text">
                  <a href="https://github.com/paul-tqh-nguyen/reuters_topic_labelling/blob/master/preprocessed_data/all_extracted_data.csv"><code>all_extracted_data.csv</code></a> contains all the data present in the <code>.sgm</code> files in a <code>.csv</code> format. A row is generated for each article, i.e. <code>&lt;REUTERS&gt;</code> element, that has sufficient data (some <code>&lt;REUTERS&gt;</code> elements are corrupt or empty). 
               </p>
               <p class="common-body-text">
                  <a href="https://github.com/paul-tqh-nguyen/reuters_topic_labelling/blob/master/preprocessed_data/topics_data.csv"><code>topics_data.csv</code></a> contains the <a href="https://en.wikipedia.org/wiki/Database_normalization">normalized</a> <code>&lt;TOPICS&gt;</code> data along with the corresponding article. 
               </p>
               <p class="common-body-text">
                  The dataset pre-processing pipeline is:
               <ol>
                  <li>Extract all of the <code>&lt;REUTERS&gt;</code> elements from the 22 <code>.sgm</code> files.</li>
                  <li>
                     From each <code>&lt;REUTERS&gt;</code> article, extract:
                     <ul style="padding-top: 20px">
                        <li style="margin-left: -10vw; font-size: 20px; margin-bottom: 5px;">the article date via <code>&lt;DATE&gt;</code>.</li>
                        <li style="margin-left: -10vw; font-size: 20px; margin-bottom: 5px;">the list of a topics (a string with comma separated values, e.g. <code>"crude, ship, oil"</code>) via <code>&lt;TOPICS&gt;</code>.</li>
                        <li style="margin-left: -10vw; font-size: 20px; margin-bottom: 5px;">the list of places mentioned in the article via <code>&lt;PLACES&gt;</code>.</li>
                        <li style="margin-left: -10vw; font-size: 20px; margin-bottom: 5px;">the list of people mentioned in the article via <code>&lt;PEOPLE&gt;</code>.</li>
                        <li style="margin-left: -10vw; font-size: 20px; margin-bottom: 5px;">the list of organizations mentioned in the article via <code>&lt;ORGS&gt;</code>.</li>
                        <li style="margin-left: -10vw; font-size: 20px; margin-bottom: 5px;">the list of companies mentioned in the article via <code>&lt;COMPANIES&gt;</code>.</li>
                        <li style="margin-left: -10vw; font-size: 20px; margin-bottom: 5px;">the article title via <code>&lt;TITLE&gt;</code>.</li>
                        <li style="margin-left: -10vw; font-size: 20px; margin-bottom: 5px;">the article <a href="https://en.wikipedia.org/wiki/Dateline">dateline</a> via <code>&lt;DATELINE&gt;</code>.</li>
                        <li style="margin-left: -10vw; font-size: 20px; margin-bottom: 5px;">the raw original article text via <code>&lt;TEXT&gt;</code>.</li>
                        <li style="margin-left: -10vw; font-size: 20px; margin-bottom: 5px;">the preprocessed article text (I'll cover this in detail momentarily).</li>
                        <li style="margin-left: -10vw; font-size: 20px; margin-bottom: 5px;">whatever arbitrary content is in the <code>&lt;UNKNOWN&gt;</code> element.</li>
                     </ul>
                  </li>
                  <li>The above data (along with the ordinal position of the <code>&lt;REUTERS&gt;</code> element within the <code>.sgm</code> file and the name of the relevant <code>.sgm</code> file) is stored in <a href="https://github.com/paul-tqh-nguyen/reuters_topic_labelling/blob/master/preprocessed_data/all_extracted_data.csv"><code>all_extracted_data.csv</code></a>. <code>&lt;REUTERS&gt;</code> elements with an empty or corrupt <code>&lt;TEXT&gt;</code> sub-element are not included.</li>
                  <li>The topics data is then <a href="https://en.wikipedia.org/wiki/Database_normalization">normalized</a> to create <a href="https://github.com/paul-tqh-nguyen/reuters_topic_labelling/blob/master/preprocessed_data/topics_data.csv"><code>topics_data.csv</code></a>. Topics with an sufficient number of associated articles were removed. Each topic was forced to have at least 200 articles as most models are incapable of learning to associate topics given an insufficient number of samples.</li>
               </ol>
               </p>
               <p class="common-body-text">
                  The article text pre-processining pipeline is (in order):
               <ol>
                  <li>Lower case the string.</li>
                  <li>Fix common typos, e.g. "<code>"s</code>" was often used in place of "<code>'s</code>" in the raw text.</li>
                  <li><a href="https://en.wikipedia.org/wiki/Ellipsis">Ellipsis</a> collapsing (i.e. replacing all text matching the reguular expression <code>"\.\.\.\.+"</code> with the string <code>"..."</code>).</li>
                  <li>Replacing numbers with a special <code>NUMBER</code> token (in order to decrease the unknown token count).</li>
                  <li><a href="https://en.wikipedia.org/wiki/Contraction_(grammar)">Contraction</a> expansion.</li>
                  <li>White space stripping.</li>
                  <li><a href="https://en.wikipedia.org/wiki/Stop_words">Stopword</a> removal.</li>
                  <li><a href="https://en.wikipedia.org/wiki/Lemmatisation">Lemmatization</a>.</li>
               </ol>
               </p>
               <p class="common-body-text">
                  Here's a sample of <a href="https://github.com/paul-tqh-nguyen/reuters_topic_labelling/blob/master/preprocessed_data/all_extracted_data.csv"><code>all_extracted_data.csv</code></a>:
               <div style="margin-left: 15vw; width: 70vw; overflow-x: scroll; border: 1px solid #999;">
                  <table class="pandas-dataframe">
                     <thead>
                        <tr style="text-align: right;">
                           <th></th>
                           <th>date</th>
                           <th>topics_raw_string</th>
                           <th>places</th>
                           <th>people</th>
                           <th>orgs</th>
                           <th>exchanges</th>
                           <th>companies</th>
                           <th>unknown</th>
                           <th>text_title</th>
                           <th>text_dateline</th>
                           <th>raw_text</th>
                           <th>text</th>
                           <th>file</th>
                           <th>reuter_element_position</th>
                        </tr>
                     </thead>
                     <tbody>
                        <tr>
                           <th>0</th>
                           <td>2-JUN-1987 10:04:07.81</td>
                           <td>['cpi']</td>
                           <td>['brazil']</td>
                           <td>['sarney']</td>
                           <td>[]</td>
                           <td>[]</td>
                           <td>[]</td>
                           <td>[" \n\x05\x05\x05C\n\x16\x16\x01f1027\x1freute...</td>
                           <td>BRAZIL'S SARNEY RENEWS CALL FOR WAR ON INFLATION</td>
                           <td>BRASILIA, June 2 -</td>
                           <td>President Jose Sarney today declared "a\nwar w...</td>
                           <td>president jose sarney today declare "a war wit...</td>
                           <td>./data/reut2-018.sgm</td>
                           <td>0</td>
                        </tr>
                        <tr>
                           <th>1</th>
                           <td>2-JUN-1987 10:05:19.67</td>
                           <td>[]</td>
                           <td>['usa']</td>
                           <td>[]</td>
                           <td>[]</td>
                           <td>[]</td>
                           <td>[]</td>
                           <td>[' \n\x05\x05\x05V RM\n\x16\x16\x01f1034\x1fre...</td>
                           <td>U.S. HOME SALES ROSE 7.6 PCT IN APRIL</td>
                           <td>WASHINGTON, June 2 -</td>
                           <td>Sales of new, single-family homes rose\n7.6 pc...</td>
                           <td>sale new, single-family home rise NUMBER . NUM...</td>
                           <td>./data/reut2-018.sgm</td>
                           <td>2</td>
                        </tr>
                        <tr>
                           <th>2</th>
                           <td>2-JUN-1987 10:06:06.42</td>
                           <td>[]</td>
                           <td>['uk']</td>
                           <td>[]</td>
                           <td>[]</td>
                           <td>[]</td>
                           <td>[]</td>
                           <td>[' \n\x05\x05\x05RM\n\x16\x16\x01f1036\x1freut...</td>
                           <td>BRIXTON ESTATE LAUNCHES UNLIMITED STG CP PROGRAM</td>
                           <td>LONDON, June 2 -</td>
                           <td>Brixton Estate Plc is establishing a\nsterling...</td>
                           <td>brixton estate plc establish sterling commerci...</td>
                           <td>./data/reut2-018.sgm</td>
                           <td>3</td>
                        </tr>
                        <tr>
                           <th>3</th>
                           <td>2-JUN-1987 10:06:53.99</td>
                           <td>[]</td>
                           <td>['usa']</td>
                           <td>['reagan', 'volcker', 'greenspan', 'james-baker']</td>
                           <td>[]</td>
                           <td>[]</td>
                           <td>[]</td>
                           <td>[' \n\x05\x05\x05V RM\n\x16\x16\x01f1037\x1fre...</td>
                           <td>REAGAN SAYS VOLCKER WILL NOT SERVE NEW TERM</td>
                           <td>WASHINGTON, June 2 -</td>
                           <td>President Reagan said Paul Volcker has\ndeclin...</td>
                           <td>president reagan say paul volcker decline serv...</td>
                           <td>./data/reut2-018.sgm</td>
                           <td>4</td>
                        </tr>
                        <tr>
                           <th>4</th>
                           <td>2-JUN-1987 10:11:04.12</td>
                           <td>[]</td>
                           <td>['belgium']</td>
                           <td>[]</td>
                           <td>[]</td>
                           <td>[]</td>
                           <td>[]</td>
                           <td>[' \n\x05\x05\x05RM\n\x16\x16\x01f1051\x1freut...</td>
                           <td>BELGIAN PUBLIC SPENDING DEFICIT FALLS IN MAY</td>
                           <td>BRUSSELS, June 2 -</td>
                           <td>Belgium's public expenditure deficit\nfell sha...</td>
                           <td>belgium's public expenditure deficit fell shar...</td>
                           <td>./data/reut2-018.sgm</td>
                           <td>5</td>
                        </tr>
                     </tbody>
                  </table>
               </div>
               </p>
               <p class="common-body-text">
                  There were 19,042 rows with sufficient and non-corrupt article text data. 
               </p>
               <p class="common-body-text">
                  There are 14 columns in <a href="https://github.com/paul-tqh-nguyen/reuters_topic_labelling/blob/master/preprocessed_data/all_extracted_data.csv"><code>all_extracted_data.csv</code></a> that correspond to what was discussed in the dataset pre-processing pipeline overview above. 
               </p>
               <p class="common-body-text">
                  Note that some columns contain strings that denote via comma-separation multiple values. The data in these columns are <a href="https://en.wikipedia.org/wiki/Database_normalization">unnormalized</a>. The topics are stored in such a column. <a href="https://github.com/paul-tqh-nguyen/reuters_topic_labelling/blob/master/preprocessed_data/topics_data.csv"><code>topics_data.csv</code></a> contains the <a href="https://en.wikipedia.org/wiki/Database_normalization">normalized</a> topic data.
               </p>
               <p class="common-body-text">
                  Here's a sample of <a href="https://github.com/paul-tqh-nguyen/reuters_topic_labelling/blob/master/preprocessed_data/topics_data.csv"><code>topics_data.csv</code></a>:
               <div style="margin-left: 15vw; width: 70vw; overflow-x: scroll; border: 1px solid #999;">
                  <table class="pandas-dataframe">
                     <thead>
                        <tr style="text-align: right;">
                           <th></th>
                           <th>text</th>
                           <th>reuter_element_position</th>
                           <th>raw_text</th>
                           <th>text_title</th>
                           <th>text_dateline</th>
                           <th>date</th>
                           <th>file</th>
                           <th>money-fx</th>
                           <th>acq</th>
                           <th>grain</th>
                           <th>earn</th>
                           <th>corn</th>
                           <th>interest</th>
                           <th>trade</th>
                           <th>crude</th>
                           <th>ship</th>
                           <th>wheat</th>
                        </tr>
                     </thead>
                     <tbody>
                        <tr>
                           <th>0</th>
                           <td>newly-nominated federal reserve board chairman...</td>
                           <td>11</td>
                           <td>Newly-nominated Federal Reserve Board\nchairma...</td>
                           <td>GREENSPAN SEES EVIDENCE DOLLAR FALL OVER</td>
                           <td>WASHINGTON, June 2 -</td>
                           <td>2-JUN-1987 10:21:40.24</td>
                           <td>./data/reut2-018.sgm</td>
                           <td>True</td>
                           <td>NaN</td>
                           <td>NaN</td>
                           <td>NaN</td>
                           <td>NaN</td>
                           <td>NaN</td>
                           <td>NaN</td>
                           <td>NaN</td>
                           <td>NaN</td>
                           <td>NaN</td>
                        </tr>
                        <tr>
                           <th>1</th>
                           <td>acme precision product inc say management grou...</td>
                           <td>16</td>
                           <td>Acme Precision Products Inc said a\nmanagement...</td>
                           <td>ACME PRECISION &lt;ACL&gt; BUYOUT BID DROPPED</td>
                           <td>DETROIT, June 2 -</td>
                           <td>2-JUN-1987 10:29:31.47</td>
                           <td>./data/reut2-018.sgm</td>
                           <td>NaN</td>
                           <td>True</td>
                           <td>NaN</td>
                           <td>NaN</td>
                           <td>NaN</td>
                           <td>NaN</td>
                           <td>NaN</td>
                           <td>NaN</td>
                           <td>NaN</td>
                           <td>NaN</td>
                        </tr>
                        <tr>
                           <th>2</th>
                           <td>italy's barley crop generally good condition h...</td>
                           <td>23</td>
                           <td>Italy's barley crop is generally in good\ncond...</td>
                           <td>ITALIAN BARLEY CROP REPORTED IN GOOD CONDITION</td>
                           <td>ROME, June 2 -</td>
                           <td>2-JUN-1987 10:35:06.58</td>
                           <td>./data/reut2-018.sgm</td>
                           <td>NaN</td>
                           <td>NaN</td>
                           <td>True</td>
                           <td>NaN</td>
                           <td>NaN</td>
                           <td>NaN</td>
                           <td>NaN</td>
                           <td>NaN</td>
                           <td>NaN</td>
                           <td>NaN</td>
                        </tr>
                        <tr>
                           <th>3</th>
                           <td>may two net shr NUMBER cts vs eight cts net NU...</td>
                           <td>27</td>
                           <td>May Two net\n    Shr 11 cts vs eight cts\n    ...</td>
                           <td>PEP BOYS - MANNY, MOE AND JACK INC &lt;PBY&gt; 1ST QTR</td>
                           <td>PHILADELPHIA, June 2 -</td>
                           <td>2-JUN-1987 10:39:23.41</td>
                           <td>./data/reut2-018.sgm</td>
                           <td>NaN</td>
                           <td>NaN</td>
                           <td>NaN</td>
                           <td>True</td>
                           <td>NaN</td>
                           <td>NaN</td>
                           <td>NaN</td>
                           <td>NaN</td>
                           <td>NaN</td>
                           <td>NaN</td>
                        </tr>
                        <tr>
                           <th>4</th>
                           <td>elder ixl ltd &lt;elxa.s&gt; say happy leave prefere...</td>
                           <td>28</td>
                           <td>Elders IXL Ltd &lt;ELXA.S&gt; says it is happy\nto l...</td>
                           <td>ELDERS HAPPY TO LEAVE CARLING SHARES OUTSTANDING</td>
                           <td>TORONTO, June 2 -</td>
                           <td>2-JUN-1987 10:40:26.29</td>
                           <td>./data/reut2-018.sgm</td>
                           <td>NaN</td>
                           <td>True</td>
                           <td>NaN</td>
                           <td>NaN</td>
                           <td>NaN</td>
                           <td>NaN</td>
                           <td>NaN</td>
                           <td>NaN</td>
                           <td>NaN</td>
                           <td>NaN</td>
                        </tr>
                     </tbody>
                  </table>
               </div>
               </p>
               <p class="common-body-text">
                  There were only 8,599 rows with sufficient data for <a href="https://github.com/paul-tqh-nguyen/reuters_topic_labelling/blob/master/preprocessed_data/topics_data.csv"><code>topics_data.csv</code></a>. This is due to the fact that many of the articles had topics with an insufficient article count (threshold of 200). An insufficient article count would make the development of a well-performing model challenging if not impossible. 
               </p>
               <p class="common-body-text">
                  Most of the data in the original dataset was sparse w.r.t. topic content. There were 10 topics that had a sufficient number of articles.
               </p>
               <h3>Problem Selection</h3>
               <p class="common-body-text">
                  There are many problem to choose from, e.g. predicting the article title given the article text, extracting the names of the mentioned people or organizations from the text, or generating the dateline given the article text. 
               </p>
               <p class="common-body-text">
                  The problem I chose for this dataset was multi-label topic classification. The goal was to develop a model to predict the possibly many topics associated with each article. This is a <a href="https://en.wikipedia.org/wiki/Multi-label_classification">multi-label classification problem</a>. 
               </p>
               <p class="common-body-text">
                  Here's an example of the desired product:
               </p>
               <center id="desired-interface-table">
                  <table>
                     <tr>
                        <td style="width: 60%">
                           <pre>
			<code>
Farmers enrolled over 6.5 mln acres
of program crops in the latest conservation reserve program
signup and around four mln acres of non-program crops,
Agriculture Department conservation specialists said.
    Soybean acreage amounted to less than two mln acres of the
non-program crop acreage enrolled, a USDA analyst said. Heavy
enrollment of non-base acreage in wheat states, of which a big
percentage would be fallow and non-soybean land, accounted for
a large portion of the non-program acreage, the analyst said.
    Wheat and corn acreage comprised slightly over 40 pct of
the total 10,572,402 acres accepted into the ten-year program.
    USDA analysts gave the following enrollment breakdown:
    -- wheat  2,615,140 acres
    -- corn   1,894,764 acres
    -- barley   705,888 acres
    -- sorghum  585,552 acres
    -- cotton   417,893 acres
    -- rice       2,035 acres
    -- peanuts      611 acres
    -- tobacco      285 acres
    -- total program crops  6,512,700 acres
    -- total nonprogram     4,059,702 acres
    -- total enrollment    10,572,402 acres
    USDA analysts are currently working on a complete state
breakdown of crop acreage enrollment and should have it ready
for publication later this week, they said.
 Reuter
			</code>
		      </pre>
                        </td>
			<td style="text-align: center;font-size:10vw; color: darkviolet;">&#8649;</td>
                        <td>
                           <pre>
			<code>
     acq <strong style="color: grey;">False</strong>
    corn <strong style="color: purple;">True</strong>
   crude <strong style="color: grey;">False</strong>
    earn <strong style="color: grey;">False</strong>
   grain <strong style="color: purple;">True</strong>
interest <strong style="color: grey;">False</strong>
money-fx <strong style="color: grey;">False</strong>
    ship <strong style="color: grey;">False</strong>
   trade <strong style="color: grey;">False</strong>
   wheat <strong style="color: purple;">True</strong>
			</code>
		      </pre>
                        </td>
                     </tr>
                     <tr>
                        <td>
                           <pre>
			<code>
Sen. David Pryor, D-Ark., said
he was considering amending the Senate Finance Committee's
trade bill with a provision to require a marketing loan for
soybeans, corn and wheat.
    Pryor told the Futures Industry Association that there was
great reluctance among members of the Senate Agriculture
Committee to reopen the 1985 farm bill, and that a marketing
loan might have a better chance in the Finance panel.
    The Arkansas senator said the marketing loan -- which in
effect allows producers to pay back their crop loans at the
world price -- had led to a 300 pct increase in U.S. cotton
exports in 14 months and a 72 pct increase in rice exports.
    Pryor serves on both the Senate Finance and Agriculture
Committees.
 Reuter
			</code>
		      </pre>
                        </td>
                        <td style="text-align: center;font-size:10vw; color: darkviolet;">&#8649;</td>
                        <td>
                           <pre>
			<code>
     acq <strong style="color: grey;">False</strong>
    corn <strong style="color: purple;">True</strong>
   crude <strong style="color: grey;">False</strong>
    earn <strong style="color: grey;">False</strong>
   grain <strong style="color: purple;">True</strong>
interest <strong style="color: grey;">False</strong>
money-fx <strong style="color: grey;">False</strong>
    ship <strong style="color: grey;">False</strong>
   trade <strong style="color: purple;">True</strong>
   wheat <strong style="color: purple;">True</strong>
			</code>
		      </pre>
                        </td>
                     </tr>
                  </table>
               </center>
               <p class="common-body-text">
                  More details about the methods used to solve this problem come in the <a href="#methods-employed">next section</a>.
               </p>
            </div>
         </div>
         <div id="methods-employed" class="topic-section">
            <center>
               <h2 class="common-UppercaseTitle">
                  <svg class="section-icon" >
                     <circle fill="#eeb5f7" cx="33" cy="33" r="33">
                     </circle>
                     <path d="M38.4 15l1-3h1l1.2 3c.2.2.5.2.7.3l2.2-2.5 1 .4-.2 3.3c.2 0 .3.2.5.4l3-1.5.7.7-1.4 3 .5.5h3.3l.4.8-2.5 2.2c0 .2 0 .5.2.7l3 1v1l-3 1.2-.3.8 2.5 2-.4 1-3.3-.2-.4.7 1.5 2.8-.7.7-3-1.4c0 .2-.4.4-.6.5l.2 3.3-1 .4-2-2.5c-.3 0-.6 0-1 .2l-1 3h-1l-1-3c-.2-.2-.5-.2-.8-.3l-2 2.5-1-.4.2-3.3-.7-.4-2.8 1.5-.7-.7 1.4-3c-.2 0-.4-.4-.5-.6l-3.3.2-.4-1 2.5-2c0-.3 0-.6-.2-1l-3-1v-1l3-1c.2-.2.2-.4.3-.7l-2.5-2.2.4-1 3.3.2c0-.2.2-.3.4-.5l-1.5-3 .7-.7 3 1.4.5-.5v-3.3l.8-.4 2.2 2.5s.5 0 .7-.2z" fill="#a784e0" transform="rotate(3.999359999999986 40 25)">
                     </path>
                     <circle fill="#eeb5f7" cx="40" cy="25" r="2">
                     </circle>
                     <path d="M21.6 26.8L19 25l-1.3 1 1.4 3c0 .2-.3.4-.5.6l-3-.8-1 1.4 2.4 2.3-.4.8-3.2.3-.3 1.6 3 1.4v.8l-3 1.4.4 1.6 3.2.3c0 .3.2.5.3.8l-2.4 2.3.8 1.4 3-.8.7.6-1.3 3 1.3 1 2.6-1.8c.3 0 .5.3.8.4l-.3 3.2 1.6.6 2-2.7c.2 0 .5 0 .7.2l1 3h1.5l1-3c0-.2.4-.2.7-.3l2 2.7 1.4-.6-.4-3.2c.3 0 .5-.3.8-.4L37 49l1.3-1-1.4-3c0-.2.3-.4.5-.6l3 .8 1-1.4-2.4-2.3.4-.8 3.2-.3.3-1.6-3-1.4v-.8l3-1.4-.4-1.6-3.2-.3c0-.3-.2-.5-.3-.8l2.4-2.3-.8-1.4-3 .8-.7-.6 1.3-3-1.3-1-2.6 1.8c-.3 0-.5-.3-.8-.4l.3-3.2-1.6-.6-2 2.7c-.2 0-.5 0-.7-.2l-1-3h-1.5l-1 3c0 .2-.4.2-.7.3l-2-2.7-1.4.6.4 3.2c-.3 0-.5.3-.8.4z" fill="#8e80ff" transform="rotate(-3.999359999999986 28 37)">
                     </path>
                     <circle fill="#eeb5f7" cx="28" cy="37" r="3">
                     </circle>
                  </svg>
                  <span>
                  Methods Employed & Performance Evaluation
                  </span>
               </h2>
            </center>
            <div style="opacity:0.05;">
               <div id="stripes" style="background: blueviolet;"></div>
            </div>
            <h3>Evaluation Method</h3>
            <p class="common-body-text">
               This is a <a href="https://en.wikipedia.org/wiki/Multi-label_classification">multi-label text classification</a> problem.
            </p>
            <p class="common-body-text">
               Most articles in the dataset have only a few topics. If an article has a few topics, then there are few positive choices and many negative choices. Thus, there is a <a href="https://en.wikipedia.org/wiki/Oversampling_and_undersampling_in_data_analysis">dataset imbalance</a>. Because of this data imbalance, the <a href="https://en.wikipedia.org/wiki/F1_score">F1 metric</a> was chosen to evaluate the performance. 
            </p>
            <h3>Model Results</h3>
            <p class="common-body-text">
               I implemented 3 neural net models, an <a href="https://en.wikipedia.org/wiki/Recursive_neural_network">RNN</a>, a <a href="https://en.wikipedia.org/wiki/Convolutional_neural_network">CNN</a>, and a <a href="https://en.wikipedia.org/wiki/Multilayer_perceptron">dense neural network</a>. 
            </p>
            <p class="common-body-text">
               I performed <a href="https://en.wikipedia.org/wiki/Random_search">random search</a> over the <a href="https://en.wikipedia.org/wiki/Hyperparameter">hyperparameter space</a> to determine the best possible score for each model.
            </p>
            <p class="common-body-text">
               I <a href="https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets">split dataset</a> to allocate 50% of examples for training, 20% of examples for validation, and 30% of examples for testing.
            </p>
            <p class="common-body-text">
               The <a href="https://en.wikipedia.org/wiki/Convolutional_neural_network">CNN</a> performed the best and got a test set F1 score of <code>0.7655879</code>.
            </p>
            <p class="common-body-text">
               The <a href="https://en.wikipedia.org/wiki/Multilayer_perceptron">dense neural network</a> did not perform as well and got a test set F1 score of <code>0.6601627</code>.
            </p>
            <p class="common-body-text">
               The <a href="https://en.wikipedia.org/wiki/Recursive_neural_network">RNN</a> performed very poorly a test set F1 score of <code>0.1065762</code>.
            </p>
            <h3>Neural Network Architectures</h3>
            <p class="common-body-text">
               I'll go over the specifics of the neural network architectures used. 
            </p>
            <p class="common-body-text">
               All of the models used <a href="https://en.wikipedia.org/wiki/Word_embedding">pre-trained word embeddings</a>. Choice of pre-trained word embedding is one of the hyperparameters optimized via random search. The pre-trained word embedding options are shown below:
            <ul>
               <li><a href="https://github.com/hassyGo/charNgram2vec">100-dimensional charNgram2vec embeddings</a> (<a href="https://arxiv.org/abs/1611.01587">relevant research paper</a>).</li>
               <li><a href="https://fasttext.cc/docs/en/pretrained-vectors.html">300-dimensional fastText skip-gram embeddings trained on Simple English Wikipedia</a> (<a href="https://arxiv.org/abs/1607.04606">relevant research paper</a>).</li>
               <li><a href="https://fasttext.cc/docs/en/pretrained-vectors.html">300-dimensional fastText skip-gram embeddings trained on English Wikipedia</a> (<a href="https://arxiv.org/abs/1607.04606">relevant research paper</a>).</li>
               <li><a href="https://nlp.stanford.edu/projects/glove/">25-dimensional GloVe embeddings trained on Twitter</a> (<a href="https://nlp.stanford.edu/pubs/glove.pdf">relevant research paper</a>).</li>
               <li><a href="https://nlp.stanford.edu/projects/glove/">50-dimensional GloVe embeddings trained on Twitter</a> (<a href="https://nlp.stanford.edu/pubs/glove.pdf">relevant research paper</a>).</li>
               <li><a href="https://nlp.stanford.edu/projects/glove/">100-dimensional GloVe embeddings trained on Twitter</a> (<a href="https://nlp.stanford.edu/pubs/glove.pdf">relevant research paper</a>).</li>
               <li><a href="https://nlp.stanford.edu/projects/glove/">200-dimensional GloVe embeddings trained on Twitter</a> (<a href="https://nlp.stanford.edu/pubs/glove.pdf">relevant research paper</a>).</li>
               <li><a href="https://nlp.stanford.edu/projects/glove/">300-dimensional GloVe embeddings trained on CommonCrawl with 42 billion tokens</a> (<a href="https://nlp.stanford.edu/pubs/glove.pdf">relevant research paper</a>).</li>
               <li><a href="https://nlp.stanford.edu/projects/glove/">300-dimensional GloVe embeddings trained on CommonCrawl with 840 billion tokens</a> (<a href="https://nlp.stanford.edu/pubs/glove.pdf">relevant research paper</a>).</li>
               <li><a href="https://nlp.stanford.edu/projects/glove/">50-dimensional GloVe embeddings trained on English Wikipedia</a> (<a href="https://nlp.stanford.edu/pubs/glove.pdf">relevant research paper</a>).</li>
               <li><a href="https://nlp.stanford.edu/projects/glove/">100-dimensional GloVe embeddings trained on English Wikipedia</a> (<a href="https://nlp.stanford.edu/pubs/glove.pdf">relevant research paper</a>).</li>
               <li><a href="https://nlp.stanford.edu/projects/glove/">200-dimensional GloVe embeddings trained on English Wikipedia</a> (<a href="https://nlp.stanford.edu/pubs/glove.pdf">relevant research paper</a>).</li>
               <li><a href="https://nlp.stanford.edu/projects/glove/">300-dimensional GloVe embeddings trained on English Wikipedia</a> (<a href="https://nlp.stanford.edu/pubs/glove.pdf">relevant research paper</a>).</li>
            </ul>
            </p>
            <p class="common-body-text">
               Below is a description of the <a href="https://en.wikipedia.org/wiki/Convolutional_neural_network">CNN</a>'s architecture:
            <ol>
               <li>The first layer was an <a href="https://en.wikipedia.org/wiki/Word_embedding">embedding layer</a>.</li>
               <li>The second layer was a set of several parallel <a href="https://en.wikipedia.org/wiki/Convolutional_neural_network#Convolutional">convolutional layers</a> that the sequence of embeddings were passed through.</li>
               <li>The third layer was a <a href="https://en.wikipedia.org/wiki/Convolutional_neural_network#Pooling">pooling layer</a> (the choice of max or average pooling was a hyperparameter) that pooled the results of each <a href="https://en.wikipedia.org/wiki/Convolutional_neural_network#Convolutional">convolutional layer</a>.</li>
               <li>The final layer was a <a href="https://en.wikipedia.org/wiki/Convolutional_neural_network#Fully_connected">fully connected layer</a> that took the concatenation of all the pooling results.</li>
            </ol>
            </p>
            <p class="common-body-text">
               Below is a description of the <a href="https://en.wikipedia.org/wiki/Recursive_neural_network">RNN</a>'s architecture:
            <ol>
               <li>The first layer was an <a href="https://en.wikipedia.org/wiki/Word_embedding">embedding layer</a>.</li>
               <li>The second layer was a bi-directional <a href="https://en.wikipedia.org/wiki/Long_short-term_memory">LSTM</a> encoding layer.</li>
               <li>The third layer was an attention layer using <a href="https://arxiv.org/abs/1703.03130">Zhouhan Lin's attention mechanism</a>.</li>
               <li>The final layer was a <a href="https://en.wikipedia.org/wiki/Convolutional_neural_network#Fully_connected">fully connected layer</a>.</li>
            </ol>
            </p>
            <p class="common-body-text">
               Below is a description of the <a href="https://en.wikipedia.org/wiki/Multilayer_perceptron">dense neural network</a>'s architecture:
            <ol>
               <li>Prior to going through any layers, the input sequence was truncated or padded to a max sequence length (a hyperparameter).</li>
               <li>The first layer was an <a href="https://en.wikipedia.org/wiki/Word_embedding">embedding layer</a>.</li>
               <li>The intermediate layers were all <a href="https://en.wikipedia.org/wiki/Convolutional_neural_network#Fully_connected">fully connected layers</a> (the size of each layer and number of layers were hyperparameters).</li>
               <li>The final layer was a <a href="https://en.wikipedia.org/wiki/Convolutional_neural_network#Fully_connected">fully connected layer</a>.</li>
            </ol>
            </p>
            <p class="common-body-text">
               Some stats regarding the best CNN are:
            <center>
               <table id="hyperparameter-stats">
                  <tr>
                     <th>Property</th>
                     <th>Value</th>
                  </tr>
                  <tr>
                     <td>Number of Epochs to Convergence</td>
                     <td>16</td>
                  </tr>
                  <tr>
                     <td>Parameter Count</td>
                     <td>6,259,190</td>
                  </tr>
                  <tr>
                     <td>Batch Size</td>
                     <td>64</td>
                  </tr>
                  <tr>
                     <td>Vocab Size</td>
                     <td>15,697</td>
                  </tr>
                  <tr>
                     <td>Pre-trained Word Embedding</td>
                     <td><a href="https://fasttext.cc/docs/en/pretrained-vectors.html">300D fastText English Wikipedia skip-gram</a></td>
                  </tr>
                  <tr>
                     <td>Convolution Layer Hidden Size</td>
                     <td>256</td>
                  </tr>
                  <tr>
                     <td>Parallel Convolution Layers' Kernel Sizes</td>
                     <td>[2, 3, 4, 5, 6]</td>
                  </tr>
                  <tr>
                     <td>Pooling Method</td>
                     <td>Max Pooling</td>
                  </tr>
                  <tr>
                     <td>Dropout Probability</td>
                     <td>0.5</td>
                  </tr>
                  <tr>
                     <td>Validation Loss</td>
                     <td>0.04981713553481713</td>
                  </tr>
                  <tr>
                     <td>Test F1</td>
                     <td>0.765587901627576</td>
                  </tr>
                  <tr>
                     <td>Test Loss</td>
                     <td>0.05267227948125866</td>
                  </tr>
               </table>
            </center>
            </br>
            </p>
            <p class="common-body-text">
               There is always room for more work, e.g trying several other different models (e.g. <a href="https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation">Latent Dirichlet allocation</a>) and methods as well (e.g. implementing <a href="https://arxiv.org/abs/1703.03130">Zhouhan Lins attention regularization</a>). The work presented here does not demonstrate an exhaustive search over all possible known methods and models. This is solely an evaluation of the methods described above.
            </p>
            <h3>Loss functions</h3>
            <p class="common-body-text">
               There are many ways to represent the problem for multi-label classification (some are listed <a href="https://en.wikipedia.org/wiki/Multi-label_classification#Problem_transformation_methods">here</a>). I explored two loss functions, sum of <a href="https://en.wikipedia.org/wiki/Cross_entropy">BCE</a> across all topics and <a href="https://towardsdatascience.com/the-unknown-benefits-of-using-a-soft-f1-loss-in-classification-systems-753902c0105d">Soft-F1</a>. 
            </p>
            <p class="common-body-text">
               I used both functions but found that neither seemed to lead to a better F1 score than the other.
            </p>
            <h3>Data Imbalance</h3>
            <p class="common-body-text">
               With <a href="https://en.wikipedia.org/wiki/Binary_classification">binary classification</a> problems, data imbalance can be solved easily through <a href="https://en.wikipedia.org/wiki/Oversampling_and_undersampling_in_data_analysis">oversampling or undersampling</a> as there are only two categories to balance. Sample until the two categories are the same size. For single-label <a href="https://en.wikipedia.org/wiki/Multiclass_classification">multi-class classification</a>, a similar method can be used. 
            </p>
            <p class="common-body-text">
               With <a href="https://en.wikipedia.org/wiki/Multi-label_classification">multi-label classification</a>, the problem of determining which oversampling or undersampling is not clear. 
            </p>
            <p class="common-body-text">
               Methods for handling data imbalance can be thought of as being in two general classes, generating samples over a <a href="https://en.wikipedia.org/wiki/Discrete_space">discrete space</a> and generating samples over a <a href="https://en.wikipedia.org/wiki/Continuous_function">continuous space</a>. 
            </p>
            <p class="common-body-text">
               Example methods of generating samples over a discrete space might include:
            <ul>
               <li><a href="https://en.wikipedia.org/wiki/Oversampling_and_undersampling_in_data_analysis">Random oversampling/undersampling of existing data</a></li>
               <li><a href="https://en.wikipedia.org/wiki/Convolutional_neural_network#Artificial_data">Generating data-augmented samples</a></li>
            </ul>
            </p>
            <p class="common-body-text">
               Example methods of generating samples over a continuous space might include:
            <ul>
               <li><a href="https://arxiv.org/pdf/1106.1813.pdf">SMOTE</a></li>
               <li><a href="https://sci2s.ugr.es/keel/pdf/algorithm/congreso/2008-He-ieee.pdf">ADASYN</a></li>
            </ul>
            </p>
            <p class="common-body-text">
               For <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing</a>, it seems that generating samples over a continuous space isnt always applicable, and it is not obvious from the current literature that this would be effective. One possible method of doing this might be using <a href="https://arxiv.org/pdf/1106.1813.pdf">SMOTE</a> (or some similar method) to generate synthetic sequences of <a href="https://en.wikipedia.org/wiki/Word_embedding">word embeddings</a>. Since it wasn't clear that this was a promising path, I did not prioritize exploring this option. Results from this investigation will come in the future.
            </p>
            <p class="common-body-text">
               For generating samples over a discrete space, data-augmentation is a known method that works (e.g. <a href="https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/52557">Kaggle competitors have augmented their data by translating to a different language and back to the original language</a>; text data has also been augmented via synonym replacement). Data augmentation creates a richer input dataset, but the labels may still be unbalanced, especially for a multi-label dataset.
            </p>
            <p class="common-body-text">
               As far as I could tell, selecting which samples to <a href="https://en.wikipedia.org/wiki/Oversampling_and_undersampling_in_data_analysis">oversample or undersample</a> and by how much is not clear since some samples might decrease imbalance by incrementing one underrepresented labels count but increase imbalance by incrementing an overrepresented labels count.
            </p>
            <p class="common-body-text">
               I'll below present my (possibly novel?) method of solving this problem.
            </p>
            <p class="common-body-text">
               Balancing a dataset can be thought of as an <a href="https://en.wikipedia.org/wiki/Integer_programming">integer programming problem</a>. The complexity is <a href="https://en.wikipedia.org/wiki/NP-completeness">NP-complete</a>. This problem can be approximately solved by oversampling or undersampling datapoints such that the label histogram reflects a <a href="https://en.wikipedia.org/wiki/Uniform_distribution_(continuous)">uniform distribution</a>.
            </p>
            <p class="common-body-text">
               A datapoint can occur <code>N</code> times (<code>N<1</code> leads to undersampling the datapoint; <code>N>1</code> leads to oversampling the datapoint). Different choices in <code>N</code> for each datapoint will lead to different final label distributions. 
            </p>
            <p class="common-body-text">
               How different the label distribution is from a <a href="https://en.wikipedia.org/wiki/Uniform_distribution_(continuous)">uniform distribution</a> can be calculated using the <a href="https://en.wikipedia.org/wiki/Chi-squared_test">Chi-Squared test</a>. 
            </p>
            <p class="common-body-text">
               If <code>N</code> is constrained to be an integer, this will be an <a href="https://en.wikipedia.org/wiki/NP-completeness">NP-complete</a> <a href="https://en.wikipedia.org/wiki/Integer_programming">integer programming problem</a>. If <code>N</code> is allowed to be be continuous, this problem is tractable since the <a href="https://en.wikipedia.org/wiki/Chi-squared_test">Chi-Squared test</a> is differentiable. An approximate solution can be achieved by truncating the <code>N</code> value for each sample.
            </p>
            <p class="common-body-text">
               Other differentiable objective functions can be chosen (some good ideas can be found in <a href="https://en.wikipedia.org/wiki/Exact_test">here</a>, <a href="https://en.wikipedia.org/wiki/Statistical_significance">here</a>, and <a href="https://en.wikipedia.org/wiki/Goodness_of_fit">here</a>). Exploring these options would be interesting future work. 
            </p>
            <p class="common-body-text">
               Though this was an interesting and fun idea to explore, oversampling the dataset using this method by varying amounts up to 3x the size of the original dataset while minimizing the label distributions difference from a <a href="https://en.wikipedia.org/wiki/Uniform_distribution_(continuous)">uniform distribution</a> achieved no noticeable improvement. 
            </p>
            <p class="common-body-text">
               I believe that this method might be useful for cases where <a href="https://en.wikipedia.org/wiki/Oversampling_and_undersampling_in_data_analysis">data imbalance</a> causes problems, but it might not be useful for this given dataset due to the small size where the <a href="https://en.wikipedia.org/wiki/Oversampling_and_undersampling_in_data_analysis">data imbalance</a> is not the dominating issue.
            </p>
            <h3>Data Augmentation</h3>
               <center id="preprocessing-interface-table">
                  <table style="margin-left: 0">
                     <tr>
                       <td style="text-align: left">
			 <pre>
			   <code>
japanese crusher buy NUMBER <strong style="color: purple; font-size: 1.5em;">NUMBER</strong> tonne canadian <strong style="color: purple; font-size: 1.5em;">rapeseed</strong> export business overnight may shipment, <strong style="color: purple; font-size: 1.5em;">trade</strong> source said.
			   </code>
			 </pre>
                       </td>
                     </tr>
                     <tr>
                       <td style="text-align: center; font-size: 5vw; color: darkviolet; margin: 0; height: 5vw;  -ms-transform: translateY(-10%); transform: translateY(-10%);">&#8650;&#8650;</td>
                     </tr>
                     <tr>
                       <td style="text-align: left">
			 <pre>
			   <code>
japanese crusher buy NUMBER <strong style="color: purple; font-size: 1.5em;">UNK</strong> tonne canadian <strong style="color: purple; font-size: 1.5em;">UNK</strong> export business overnight may shipment, <strong style="color: purple; font-size: 1.5em;">UNK</strong> source said.
			   </code>
			 </pre>
                       </td>
                     </tr>
		  </table>
	       </center>
            <p class="common-body-text">
               I attempted to use <a href="https://en.wikipedia.org/wiki/Convolutional_neural_network#Artificial_data">data augmentation</a> to enhance performance. 
            </p>
            <p class="common-body-text">
               The data was augmented by randomly replacing words with the <a href="https://en.wikipedia.org/wiki/Statistical_machine_translation#Out_of_vocabulary_(OOV)_words"><code>UNK</code></a> token.
            </p>
            <p class="common-body-text">
               This was tried with several probabilities (up to 70%).
            </p>
            <p class="common-body-text">
               I saw no noticeable improvement in F1 score across all of the models. 
            </p>
            <p class="common-body-text">
               The most salient change observed was that the <a href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Iterative_method">convergence</a> of the <a href="https://en.wikipedia.org/wiki/Convolutional_neural_network">CNN</a> and <a href="https://en.wikipedia.org/wiki/Multilayer_perceptron">dense neural network</a> slowed by about 2-3 epochs, but they both eventually converged to approximately what they normally would without the <a href="https://en.wikipedia.org/wiki/Convolutional_neural_network#Artificial_data">data augmentation</a>. 
            </p>
            <p class="common-body-text">
               For future work, another possible method worth exploring would be replacing words with synonyms (possibly restricting the synonym choices to only those present in the training set rather than all possible synonyms in the English language). 
            </p>
            <h3>F1 Threshold Optimization</h3>
               <center id="preprocessing-interface-table" style="padding-top: 30px; padding-bottom: 30px;">
                  <table style="margin-left: 0; ">
                     <tr>
                       <td style="text-align: center">&nbsp;</td>
                       <td style="text-align: center">&nbsp;</td>
                       <td style="text-align: center">&nbsp;</td>
                       <td style="text-align: center">&nbsp;</td>
                       <td style="text-align: center">&nbsp;</td>
                       <td style="text-align: center">&nbsp;</td>
                       <td style="text-align: center">&nbsp;</td>
                       <td style="text-align: center">&nbsp;</td>
                       <td style="text-align: center">&nbsp;</td>
                       <td style="text-align: center">&nbsp;</td>
                       <td style="text-align: center">&nbsp;</td>
                       <td style="text-align: center"><code style="font-size: 15px">Rounding</br>Threshold</code></td>
                       <td style="text-align: center">&nbsp;</td>
                       <td style="text-align: center">&nbsp;</td>
                       <td style="text-align: center">&nbsp;</td>
                       <td style="text-align: center">&nbsp;</td>
                       <td style="text-align: center">&nbsp;</td>
                       <td style="text-align: center">&nbsp;</td>
                       <td style="text-align: center">&nbsp;</td>
                       <td style="text-align: center">&nbsp;</td>
                       <td style="text-align: center">&nbsp;</td>
                       <td style="text-align: center">&nbsp;</td>
                       <td style="text-align: center">&nbsp;</td>
                     </tr>
		  </table>
                  <table style="margin-left: 0; background-color: #eee; border: 1px solid #999; ">
                    <tr>
                       <td style="text-align: left"><code style="font-size: 20px">0.0</code></td>
                       <td style="text-align: center">&nbsp;</td>
                       <td style="text-align: center"><code style="font-size: 20px">N</td>
                       <td style="text-align: center">&nbsp;</td>
                       <td style="text-align: center">&nbsp;</td>
                       <td style="text-align: center">&nbsp;</td>
                       <td style="text-align: center">&nbsp;</td>
                       <td style="text-align: center">&nbsp;</td>
                       <td style="text-align: center"><code style="font-size: 20px; color: #eee">_____</code><code style="font-size: 20px; color: darkviolet">|</code><code style="font-size: 20px; color: #eee">_____</code></td>
                       <td style="text-align: center">&nbsp;</td>
                       <td style="text-align: center">&nbsp;</td>
                       <td style="text-align: center">&nbsp;</td>
                       <td style="text-align: center"><code style="font-size: 20px">0.5</code></td>
                       <td style="text-align: center">&nbsp;</td>
                       <td style="text-align: center">&nbsp;</td>
                       <td style="text-align: center"><code style="font-size: 20px">P</td>
                       <td style="text-align: center">&nbsp;</td>
                       <td style="text-align: center">&nbsp;</td>
                       <td style="text-align: center">&nbsp;</td>
                       <td style="text-align: center">&nbsp;</td>
                       <td style="text-align: center">&nbsp;</td>
                       <td style="text-align: center">&nbsp;</td>
                       <td style="text-align: center">&nbsp;</td>
                       <td style="text-align: right"><code style="font-size: 20px">1.0</code></td>
                     </tr>
		  </table>
                  <table style="margin-left: 0; ">
                     <tr>
                       <td style="text-align: center">&nbsp;</td>
                       <td style="text-align: center">&nbsp;</td>
                       <td style="text-align: center"><code style="font-size: 15px">Negative Prediction Mean</code></td>
                       <td style="text-align: center">&nbsp;</td>
                       <td style="text-align: center">&nbsp;</td>
                       <td style="text-align: center">&nbsp;</td>
                       <td style="text-align: center">&nbsp;</td>
                       <td style="text-align: center">&nbsp;</td>
                       <td style="text-align: center"><code style="font-size: 20px; color: darkviolet">Optimized Threshold</code></td>
                       <td style="text-align: center">&nbsp;</td>
                       <td style="text-align: center">&nbsp;</td>
                       <td style="text-align: center">&nbsp;</td>
                       <td style="text-align: center">&nbsp;</td>
                       <td style="text-align: center">&nbsp;</td>
                       <td style="text-align: center"><code style="font-size: 15px">Positive Prediction Mean</code></td>
                       <td style="text-align: center">&nbsp;</td>
                       <td style="text-align: center">&nbsp;</td>
                       <td style="text-align: center">&nbsp;</td>
                       <td style="text-align: center">&nbsp;</td>
                       <td style="text-align: center">&nbsp;</td>
                       <td style="text-align: center">&nbsp;</td>
                       <td style="text-align: center">&nbsp;</td>
                       <td style="text-align: center">&nbsp;</td>
                     </tr>
		  </table>
	       </center>
            <p class="common-body-text">
               When deep neural networks are used in practice to classify samples, the continuous output of the network is discretized frequently using a <a href="https://en.wikipedia.org/wiki/Rounding">round function</a>. This simply sets the threshold at <code>0.5</code>.
            </p>
            <p class="common-body-text">
               Inspired by the idea of threshold selection when reviewing how <a href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic">ROC curves</a> are used with <a href="https://en.wikipedia.org/wiki/Logistic_regression">logistic regression models</a>, I realized that using a threshold of <code>0.5</code> is likely sufficient but not globally optimal. 
            </p>
            <p class="common-body-text">
               A model might have a <a href="https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff">bias</a> that it cannot overcome that'll prevent the positive predictions for a label to be exactly 1 or exactly 0 for a negative prediction. Because of this, it is not clear that for a given label the positive predictions and negative predictions would be centered around <code>0.5</code>.
            </p>
            <p class="common-body-text">
               Thus, I experimented with naively optimizing the threshold for discretizing the neural network outputs. 
            </p>
            <p class="common-body-text">
               The naive method was to chose the threshold for each label to be the center point of the mean of the positive predictions for the label and the mean of the negative predictions for the label . 
            </p>
            <p class="common-body-text">
               This led to a testing F1 score increase of about <code>0.025</code> for all of the models. 
            </p>
            <p class="common-body-text">
               This small but noticeable improvement might be worth investigating further in the future by appropriately accounting for outliers in the center point calculation using a <a href="https://en.wikipedia.org/wiki/Standard_deviation">standard deviation</a> threshold. Another option worth exploring would be to exclude <a href="https://en.wikipedia.org/wiki/False_positives_and_false_negatives">false positives and false negatives</a>.
            </p>
            <h3>Stopword Removal</h3>
               <center id="preprocessing-interface-table">
                  <table style="margin-left: 0">
                     <tr>
                       <td style="text-align: left">
			 <pre>
			   <code>
arvin industries inc said l.k. evans <strong style="color: purple; font-size: 1.5em;">has been</strong> elected president, succeeding james baker <strong style="color: purple; font-size: 1.5em;">who</strong> remains chairman.
			   </code>
			 </pre>
                       </td>
                     </tr>
                     <tr>
                       <td style="text-align: center; font-size: 5vw; color: darkviolet; margin: 0; height: 5vw;  -ms-transform: translateY(-10%); transform: translateY(-10%);">&#8650;&#8650;</td>
                     </tr>
                     <tr>
                       <td style="text-align: left">
			 <pre>
			   <code>
arvin industries inc said l.k. evans elected president, succeeding james baker remains chairman.
			   </code>
			 </pre>
                       </td>
                     </tr>
		  </table>
	       </center>
            <p class="common-body-text">
               <a href="https://en.wikipedia.org/wiki/Stop_words">stopwords</a> were removed during data pre-processing. 
            </p>
            <p class="common-body-text">
               This increased the testing F1 score by about <code>0.08</code> in the CNN and dense neural network. 
            </p>
            <p class="common-body-text">
               This led to no noticeable improvement in the RNN.
            </p>
            <p class="common-body-text">
               I used <a href="https://www.nltk.org/">NLTK</a>'s English <a href="https://en.wikipedia.org/wiki/Stop_words">stopword</a> list for <a href="https://en.wikipedia.org/wiki/Stop_words">stopword</a> removal via <code>nltk.corpus.stopwords.words('english')</code>.
            </p>
            <h3>Contraction Expansion</h3>
               <center id="contraction-table">
                  <table style="margin-left: 0;">
                     <tr>
                       <td style="text-align: center"><pre><code>needn't</code></pre></td>
		       <td style="text-align: center;font-size:5vw; color: darkviolet;">&#8649;</td>
                       <td style="text-align: center"><pre><code>need not</code></pre></td>
                       <td style="text-align: center">&nbsp;</td>
                       <td style="text-align: center"><pre><code>there'd</code></pre></td>
		       <td style="text-align: center;font-size:5vw; color: darkviolet;">&#8649;</td>
                       <td style="text-align: center"><pre><code>there had</code></pre></td>
                     </tr>
                     <tr>
                       <td style="text-align: center"><pre><code>where's</code></pre></td>
		       <td style="text-align: center;font-size:5vw; color: darkviolet;">&#8649;</td>
                       <td style="text-align: center"><pre><code>where is</code></pre></td>
                       <td style="text-align: center">&nbsp;</td>
                       <td style="text-align: center"><pre><code>mightn't</code></pre></td>
		       <td style="text-align: center;font-size:5vw; color: darkviolet;">&#8649;</td>
                       <td style="text-align: center"><pre><code>might not</code></pre></td>
                     </tr>
                     <tr>
                       <td style="text-align: center"><pre><code>can't've</code></pre></td>
		       <td style="text-align: center;font-size:5vw; color: darkviolet;">&#8649;</td>
                       <td style="text-align: center"><pre><code>cannot have</code></pre></td>
                       <td style="text-align: center">&nbsp;</td>
                       <td style="text-align: center"><pre><code>they're</code></pre></td>
		       <td style="text-align: center;font-size:5vw; color: darkviolet;">&#8649;</td>
                       <td style="text-align: center"><pre><code>they are</code></pre></td>
                     </tr>
                     <tr>
                       <td style="text-align: center"><pre><code>haven't</code></pre></td>
		       <td style="text-align: center;font-size:5vw; color: darkviolet;">&#8649;</td>
                       <td style="text-align: center"><pre><code>have not</code></pre></td>
                       <td style="text-align: center">&nbsp;</td>
                       <td style="text-align: center"><pre><code>they'd've</code></pre></td>
		       <td style="text-align: center;font-size:5vw; color: darkviolet;">&#8649;</td>
                       <td style="text-align: center"><pre><code>they would have</code></pre></td>
                     </tr>
		  </table>
	       </center>
            <p class="common-body-text">
               <a href="https://en.wikipedia.org/wiki/Contraction_(grammar)">Contractions</a> were expanded during data pre-processing. It yielded no noticeable improvement for any of the models. 
            </p>
            <p class="common-body-text">
               I manually implemented the contraction/shorthand expander. The list of contractions and shorthand can be found <a href="https://github.com/paul-tqh-nguyen/reuters_topic_labelling/blob/master/preprocess_data.py#L47">here</a>.
            </p>
            <h3>Number Tokenization</h3>
               <center id="preprocessing-interface-table">
                  <table style="margin-left: 0">
                     <tr>
                       <td style="text-align: left">
			 <pre>
			   <code>
general motors corp said it produced <strong style="color: purple; font-size: 1.5em;">395,294</strong> cars in the u.s. in march, up from <strong style="color: purple; font-size: 1.5em;">366,671</strong> in march <strong style="color: purple; font-size: 1.5em;">1986</strong>.
			   </code>
			 </pre>
                       </td>
                     </tr>
                     <tr>
                       <td style="text-align: center; font-size: 5vw; color: darkviolet; margin: 0; height: 5vw;  -ms-transform: translateY(-10%); transform: translateY(-10%);">&#8650;&#8650;</td>
                     </tr>
                     <tr>
                       <td style="text-align: left">
			 <pre>
			   <code>
general motors corp said it produced <strong style="color: purple; font-size: 1.5em;">NUMBER</strong> cars in the u.s. in march, up from <strong style="color: purple; font-size: 1.5em;">NUMBER</strong> in march <strong style="color: purple; font-size: 1.5em;">NUMBER</strong> .
			   </code>
			 </pre>
                       </td>
                     </tr>
		  </table>
	       </center>
            <p class="common-body-text">
               Numbers, e.g. <code>"9"</code>, <code>"3/4"</code>, <code>"1.3"</code>, <code>"1,256,412"</code>, were replaced with a special <code>NUMBER</code> token. While this drastically decreased the <a href="https://en.wikipedia.org/wiki/Statistical_machine_translation#Out_of_vocabulary_(OOV)_words"><code>UNK</code></a> count in the data and the total number of <a href="https://en.wikipedia.org/wiki/Statistical_machine_translation#Out_of_vocabulary_(OOV)_words">unknown words</a> (which is expected), it yielded no noticeable improvement for any of the models. It did help reduce the noise when investigating the impact of <a href="https://en.wikipedia.org/wiki/Statistical_machine_translation#Out_of_vocabulary_(OOV)_words">unknown words</a>.
            </p>
            <h3>Lemmatization</h3>
               <center id="preprocessing-interface-table">
                  <table style="margin-left: 0">
                     <tr>
                       <td style="text-align: left">
			 <pre>
			   <code>
amoco corp said <strong style="color: purple; font-size: 1.5em;">raised</strong> contract price pay <strong style="color: purple; font-size: 1.5em;">grades</strong> crude oil NUMBER cts barrel, effective today.
			   </code>
			 </pre>
                       </td>
                     </tr>
                     <tr>
                       <td style="text-align: center; font-size: 5vw; color: darkviolet; margin: 0; height: 5vw;  -ms-transform: translateY(-10%); transform: translateY(-10%);">&#8650;&#8650;</td>
                     </tr>
                     <tr>
                       <td style="text-align: left">
			 <pre>
			   <code>
amoco corp say <strong style="color: purple; font-size: 1.5em;">raise</strong> contract price pay <strong style="color: purple; font-size: 1.5em;">grade</strong> crude oil NUMBER cts barrel, effective today.
			   </code>
			 </pre>
                       </td>
                     </tr>
		  </table>
	       </center>
            <p class="common-body-text">
               I employed <a href="https://en.wikipedia.org/wiki/Lemmatisation">lemmatization</a> during the data pre-processing. It yielded no noticeable improvement for any of the models. 
            </p>
            <p class="common-body-text">
               I used <a href="https://www.nltk.org/">NLTK</a>'s <a href="https://wordnet.princeton.edu/">WordNet</a> <a href="https://en.wikipedia.org/wiki/Lemmatisation">lemmatizer</a> via <code>nltk.stem.WordNetLemmatizer()</code>.
            </p>
         </div>
         <div id="implementation-details" class="topic-section">
            <center>
               <h2 class="common-UppercaseTitle">
                  <svg class="section-icon">
                     <circle fill="#eeb5f7" cx="33" cy="33" r="33">
                     </circle>
                     <path d="M20 24c0-1.7 1-2.3 2.5-1.3l13 8.6c1.4 1 1.4 2.4 0 3.4l-13 8.6c-1.4 1-2.5.4-2.5-1.3V24" fill="#9d8fff" fill-opacity="0.8" transform="translate(-2.789979447470614e-8 1.1508664243820022e-8) scale(0.9999999996512525)">
                     </path>
                     <path d="M20 24c0-1.7 1-2.3 2.5-1.3l13 8.6c1.4 1 1.4 2.4 0 3.4l-13 8.6c-1.4 1-2.5.4-2.5-1.3V24" fill="#8e80ff" fill-opacity="1.0" transform="translate(12.001415746356617)">
                     </path>
                  </svg>
                  <span>
                  Implementation Details & Code Demonstration
                  </span>
               </h2>
            </center>
            <p class="common-body-text">
               In this section, I'll go over how to use the code that I've implemented <a href="https://github.com/paul-tqh-nguyen/reuters_topic_labelling">here</a>. Much of this information is specific to my implementation, so this section is only useful for those who might want to experiment with my code directly. 
            </p>
            <p class="common-body-text">
               I intend for the code to be fairly platform independent via the <a href="https://github.com/paul-tqh-nguyen/reuters_topic_labelling/blob/master/environment.yml">environment.yml</a>. If you're not familiar with managing environments with <a href="https://docs.conda.io/en/latest/">conda</a>, helpful documentation can be found <a href="https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html">here</a>.
            </p>
            <p class="common-body-text">
               If you have <a href="https://docs.conda.io/en/latest/">conda</a> installed, simply running <code>conda env create ; conda activate reuters</code> from the checkout directory will handle all dependency issues.
            </p>
            <p class="common-body-text">
	      You may have to download the <a href="https://www.nltk.org/">NLTK</a> <a href="https://en.wikipedia.org/wiki/Stop_words">stopword</a> set via <code>python3 -c "import nltk; nltk.download('stopwords')"</code>.
            </p>
            <p class="common-body-text">
	      You also may have to download the <a href="https://www.nltk.org/">NLTK</a> <a href="https://wordnet.princeton.edu/">WordNet</a> set via <code>python3 -c "import nltk; nltk.download('wordnet')"</code>.
            </p>
            <p class="common-body-text">
	      There may also be some <a href="https://spacy.io/"><code>spacy</code></a> utilities that need to be downloaded via <code>python3 -m spacy download en</code>.
            </p>
            <p class="common-body-text">
               If you're having trouble, <a href="mailto:paul.tqh.nguyen@gmail.com">please let me know</a>. 
            </p>
            <h3>Command-Line Interface</h3>
            <p class="common-body-text">
               Much of the functionality discussed in this document can be utilized via the <a href="https://en.wikipedia.org/wiki/Command-line_interface">CLI</a>.
            <pre>
		<code>
pnguyen@pnguyenmachine:/tmp/reuters_topic_labelling$ ./main.py
usage: main.py [-h] [-preprocess-data] [-train-model] [-hyperparameter-search] [-hyperparameter-search-rnn] [-hyperparameter-search-conv] [-hyperparameter-search-dense]

optional arguments:
  -h, --help                    show this help message and exit
  -preprocess-data              Preprocess the raw SGML files into a CSV.
  -train-model                  Trains &amp; evaluates our model on our dataset. Saves model to ./best-model.pt.
  -hyperparameter-search        Exhaustively performs -train-model over the hyperparameter space. Details of the best performance are tracked in global_best_model_score.json.
  -hyperparameter-search-rnn    Perform the hyperparameter search on our RNN model.
  -hyperparameter-search-conv   Perform the hyperparameter search on our CNN model.
  -hyperparameter-search-dense  Perform the hyperparameter search on our simple feed-forward model.
pnguyen@pnguyenmachine:/tmp/reuters_topic_labelling$ 
		</code>
	    </pre>
            </p>
            <h3>Data Pre-processing</h3>
            <p class="common-body-text">
               I used <a href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/">Beautiful Soup</a> to parse the <code>.sgm</code> files. 
            </p>
            <p class="common-body-text">
               I used <a href="https://docs.python.org/3/library/multiprocessing.html">Python's native multiprocessing library</a> to process each <code>.sgm</code> file independently and in parallel. 
            </p>
            <p class="common-body-text">
               I used <a href="https://www.nltk.org/">NLTK</a>'s English <a href="https://en.wikipedia.org/wiki/Stop_words">stopword</a> list for <a href="https://en.wikipedia.org/wiki/Stop_words">stopword</a> removal via <code>nltk.corpus.stopwords.words('english')</code>.
            </p>
            <p class="common-body-text">
               I implemented the contraction/shorthand expander manually rather than using an external library. The list of contractions and shorthand can be found <a href="https://github.com/paul-tqh-nguyen/reuters_topic_labelling/blob/master/preprocess_data.py#L47">here</a>.
            </p>
            <p class="common-body-text">
               I used <a href="https://www.nltk.org/">NLTK</a>'s <a href="https://wordnet.princeton.edu/">WordNet</a> <a href="https://en.wikipedia.org/wiki/Lemmatisation">lemmatizer</a> via <code>nltk.stem.WordNetLemmatizer()</code>.
            </p>
            <p class="common-body-text">
               One can easily instantiate the data pre-processing via the command-line interface:
            <pre>
		<code>
pnguyen@pnguyenmachine:/tmp/reuters_topic_labelling$ ./main.py -preprocess-data
Parsing .sgm files.
Parsing of .sgm files complete.
Parsing of .sgm files took 45.32427167892456 seconds.

Preprocessing of entire dataset is in ./preprocessed_data/all_extracted_data.csv
./preprocessed_data/all_extracted_data.csv has 19042 rows.
./preprocessed_data/all_extracted_data.csv has 14 columns.

Preprocessing of topics is in ./preprocessed_data/topics_data.csv
./preprocessed_data/topics_data.csv has 10 topics.
./preprocessed_data/topics_data.csv has 8599 rows.
./preprocessed_data/topics_data.csv has 17 columns.

pnguyen@pnguyenmachine:/tmp/reuters_topic_labelling$ 
		</code>
	      </pre>
            </p>
            <h3>Deep Learning Models</h3>
            <p class="common-body-text">
               I'll now go over how to use the deep learning models.
            </p>
            <p class="common-body-text">
               I used <a href="https://pytorch.org/">PyTorch</a> and <a href="https://pytorch.org/text/">TorchText</a> to implement the models.
            </p>
            <p class="common-body-text">
               I'll show how to use the <a href="https://en.wikipedia.org/wiki/Convolutional_neural_network">CNN</a> for now. Use of the other models will be analogous.
            </p>
            <p class="common-body-text">
            <pre>
		<code>
&gt;&gt;&gt; from torch.nn.functional import max_pool1d, avg_pool1d, adaptive_avg_pool1d, adaptive_max_pool1d, lp_pool1d
&gt;&gt;&gt; NUMBER_OF_EPOCHS = 100
&gt;&gt;&gt; BATCH_SIZE = 64
&gt;&gt;&gt; MAX_VOCAB_SIZE = 25_000
&gt;&gt;&gt; TRAIN_PORTION, VALIDATION_PORTION, TESTING_PORTION = (0.50, 0.20, 0.3)
&gt;&gt;&gt; PRE_TRAINED_EMBEDDING_SPECIFICATION = 'fasttext.en.300d'
&gt;&gt;&gt; CONVOLUTION_HIDDEN_SIZE = 256
&gt;&gt;&gt; KERNEL_SIZES = [3,4,5,6]
&gt;&gt;&gt; POOLING_METHOD = max_pool1d
&gt;&gt;&gt; DROPOUT_PROBABILITY = 0.5
&gt;&gt;&gt; OUTPUT_DIR = './default_output/'
&gt;&gt;&gt; 
&gt;&gt;&gt; 
&gt;&gt;&gt; 
&gt;&gt;&gt; from models import ConvClassifier
&gt;&gt;&gt; classifier = ConvClassifier(OUTPUT_DIR, NUMBER_OF_EPOCHS, BATCH_SIZE, TRAIN_PORTION, VALIDATION_PORTION, TESTING_PORTION, MAX_VOCAB_SIZE, PRE_TRAINED_EMBEDDING_SPECIFICATION, convolution_hidden_size=CONVOLUTION_HIDDEN_SIZE, kernel_sizes=KERNEL_SIZES, pooling_method=POOLING_METHOD, dropout_probability=DROPOUT_PROBABILITY)
Original dataset size: 4300
Original dataset chi-squared statistic: 6120.89306640625
Final dataset chi-squared statistic: 6120.89306640625
Oversampled dataset size: 4300
Dataset balancing took 2.1816141605377197 seconds.
&gt;&gt;&gt; classifier.train()

Model hyperparameters are:
        number_of_epochs: 100
        batch_size: 64
        max_vocab_size: 25000
        vocab_size: 15652
        pre_trained_embedding_specification: fasttext.en.300d
        output_size: 10
        output_directory: ./default_output/
        convolution_hidden_size: 256
        dropout_probability: 0.5
        kernel_sizes: [3, 4, 5, 6]
        pooling_method: max_pool1d

The model has 6089274 trainable parameters.
This processes's PID is 3275.

Starting training


Epoch 0
Training F1 0.05750538: 100%|| 68/68 [00:09&lt;00:00,  6.89it/s]
Optimizing F1 Threshold: 100%|| 68/68 [00:02&lt;00:00, 31.08it/s]
Validation F1 0.33941791: 100%|| 41/41 [00:01&lt;00:00, 34.28it/s]
	 Train F1: 0.05750538 | Train Recall: 0.04218872 | Train Precision: 0.10586598 | Train Loss: 0.47773770
	  Val. F1: 0.33941791 |  Val. Recall: 0.80459968 |  Val. Precision: 0.26790681 |  Val. Loss: 0.23443730
Optimizing F1 Threshold: 100%|| 68/68 [00:02&lt;00:00, 30.84it/s]
Testing F1 0.33346784: 100%|| 27/27 [00:01&lt;00:00, 21.71it/s]
	  Test F1: 0.33346784 |  Test Recall: 0.76290761 |  Test Precision: 0.26209607 |  Test Loss: 0.23624121
Epoch 0 took 17.25825786590576 seconds.

<strong style="font-size: 1.2em;color: red;">Skipping printing for brevity...</strong>

Epoch 16
Training F1 0.49711680: 100%|| 68/68 [00:07&lt;00:00,  9.40it/s]
Optimizing F1 Threshold: 100%|| 68/68 [00:02&lt;00:00, 26.92it/s]
Validation F1 0.81630142: 100%|| 41/41 [00:01&lt;00:00, 30.55it/s]
	 Train F1: 0.49711680 | Train Recall: 0.40129116 | Train Precision: 0.72455534 | Train Loss: 0.35573775
	  Val. F1: 0.81630142 |  Val. Recall: 0.85080909 |  Val. Precision: 0.80316160 |  Val. Loss: 0.05132463
Optimizing F1 Threshold: 100%|| 68/68 [00:02&lt;00:00, 28.46it/s]
Testing F1 0.75295177: 100%|| 27/27 [00:01&lt;00:00, 21.24it/s]
	  Test F1: 0.75295177 |  Test Recall: 0.78413611 |  Test Precision: 0.74552983 |  Test Loss: 0.05430432
Epoch 16 took 14.843393564224243 seconds.



Validation is not better than any of the 5 recent epochs, so training is ending early due to apparent convergence.

Optimizing F1 Threshold: 100%|| 68/68 [00:02&lt;00:00, 28.09it/s]
Testing F1 0.75295177: 100%|| 27/27 [00:01&lt;00:00, 20.66it/s]
	  Test F1: 0.75295177 |  Test Recall: 0.78413611 |  Test Precision: 0.74552983 |  Test Loss: 0.05430432
&gt;&gt;&gt; 
&gt;&gt;&gt; 
&gt;&gt;&gt; 
&gt;&gt;&gt; classifier.classify_string('''
The Bank of England said it had provided
the money market with 115 mln stg assistance in the morning
session. This compares with the Bank's forecast of a 300 mln
stg shortage in the system today.
    The central bank bought bills outright in band two at
9-13/16 pct comprising 73 mln stg bank bills and 42 mln stg
local authority bills.
 REUTER
''')
{'money-fx', 'interest'}
&gt;&gt;&gt; 
&gt;&gt;&gt; 
&gt;&gt;&gt; 
&gt;&gt;&gt; classifier.classify_string('''
Shr profit two cts vs loss 58 cts
    Net profit 18.2 mln vs loss 23.9 mln
    Revs 355.6 mln vs 308.2 mln
    Nine mths
    Shr loss 81 cts vs loss 5.52 dlrs
    Net profit 10.7 mln vs loss 290.3 mln
    Revs 1.01 billion vs 983.3 mln
    NOTE: Net income per share is after deductions for
mandatory preferred stock dividends and income from the
chemical operations not attributable to common stockholders.
    1987 qtr and nine mths includes gain of eight cts per share
for the partial redemption of series a preferred stock which
will be paid from the net earnings of the chemicals operations.
1986 nine mths includes loss 247.7 mln dlrs from write-down of
petroleum service assets and other restructuring costs.
 Reuter
''')
{'earn'}
&gt;&gt;&gt; 
&gt;&gt;&gt; 
&gt;&gt;&gt; 
&gt;&gt;&gt; classifier.classify_string('''
U.S. farmers who in the past have
grown oats for their own use but failed to certify to the
government that they had done so probably will be allowed to
continue planting that crop and be eligible for corn program
benefits, an aide to Agriculture Secretary Richard Lyng said.
    Currently a farmer, to be eligible for corn program
benefits, must restrict his plantings of other program crops to
the acreage base for that crop.
    Several members of Congress from Iowa have complained that
farmers who inadvertantly failed to certify that they had grown
oats for their own use in the past now are being asked to halt
oats production or lose corn program benefits.
    USDA likely will allow historic oats farmers to plant oats
but not extend the exemption to all farmers, Lyng's aide said.
 Reuter
''')
{'grain', 'corn', 'wheat'}
&gt;&gt;&gt; 
		</code>
	      </pre>
            </p>
            <p class="common-body-text">
               NB: Testing only happens when a new validation minimum is achieved and at the end of training.
            </p>
            <p class="common-body-text">
               NB: Training ends when convergence is achieved, i.e. when 5 epochs pass without a new validation minimum.
            </p>
            <p class="common-body-text">
               If you're interested in using the RNN or dense neural network, consider using the <a href="https://github.com/paul-tqh-nguyen/reuters_topic_labelling/blob/master/models.py"><code>EEAPClassifier</code></a> and <a href="https://github.com/paul-tqh-nguyen/reuters_topic_labelling/blob/master/models.py"><code>DenseClassifier</code></a> classes. Usage will be analogous to what's shown above.
            </p>
         </div>
         <div id="conclusion" class="topic-section">
            <center>
               <h2 class="common-UppercaseTitle">
                  <svg class="section-icon">
                     <circle fill="#eeb5f7" cx="33" cy="33" r="33"></circle>
                     <rect fill="none" stroke="#8e80ff" stroke-width="2" x="22" y="22" width="22" height="22" rx="1"></rect>
                     <path d="M26.10164627022045,32.09865305163782 C25.001845818125954,30.998852599543326 24.001995479055086,31.298752825590572 24.001995479055086,32.998503390708684 L24.001995479055086,39.99800452094492 C24.001995479055086,41.09800452094491 24.901995479055085,41.99800452094492 26.001995479055086,41.99800452094492 L33.001496609291316,41.99800452094492 C34.70124717440943,41.99800452094492 35.10119728743305,41.09820406885042 33.90134694836218,39.89835372977955 L31.80139683533856,37.79840361675593 L37.79840361675593,31.80139683533856 L39.89835372977955,33.90134694836218 C40.99815418187404,35.001147400456674 41.99800452094492,34.70124717440943 41.99800452094492,33.001496609291316 L41.99800452094492,26.001995479055086 C41.99800452094492,24.901995479055085 41.09800452094491,24.001995479055086 39.99800452094492,24.001995479055086 L32.998503390708684,24.001995479055086 C31.298752825590572,24.001995479055086 30.898802712566948,24.901795931149575 32.09865305163782,26.10164627022045 L34.298653051637814,28.301646270220445 L28.301646270220445,34.298653051637814 L26.10164627022045,32.09865305163782" stroke="#fde0e0" stroke-width="2" fill="#8e80ff"></path>
                  </svg>
                  <span>
                  Conclusion & Lessons Learned
                  </span>
               </h2>
            </center>
            <div style="opacity:0.05;">
               <div id="stripes" style="background: darkviolet;"></div>
            </div>
            <p class="common-body-text">
               The biggest lesson is that it is much more practical to use a <a href="https://jupyter.org/">Jupyter Notebook</a> to write up notes and findings than it is to write an HTML page. 
            </p>
            <p class="common-body-text">
               As far as the other lessons go, they fall into two <i>overlapping</i> categories, deep-learning engineering practices/theory and software engineering practices. 
            </p>
            <h3>Deep-learning Lessons</h3>
            <h4>RNNs are not the King/Queen of NLP</h4>
            <p class="common-body-text">
               RNNs arent everything. RNNs dont solve all NLP problems. It is easy to fall into the trap (that I did) of thinking that simply because text data is sequential that an RNN is the best model for the job. 
            </p>
            <p class="common-body-text">
               This is a variation on the idea that not the hottest or coolest or newest technology is the best. You always want to use the best tool for the job, not the coolest. You want to pick the best tool for the job. If you have a simpler tool that does the job better, use it. Once you have the problem, youre picking the tool, not the problem. Dont try to force the tool to solve the problem. Sometimes, it simply cant. Its like trying to fit a square peg into a round hole. Even if it slides in, it might not fit the hole best. 
            </p>
            <p class="common-body-text">
               I did this when trying to use LSTM+Attention. I spent a lot of time trying to figure out what was wrong and seeing if there were any bugs. I was getting a lot of bad performance and marginal returns with all my efforts and new ideas (e.g. the idea to optimize the F1 threshold; decent idea but minimal gains). Once I tried other simpler and less cool methods, e.g. the CNN, I got way better results. 
            </p>
            <h4>Start with simpler models and only use more complicated models when motivated</h4>
            <p class="common-body-text">
               This is very much in line with the principles of extreme programming. Its useful to only spend all the time and effort to create a really complicated model if it is necessary. If it turns out to not be necessary (and even lead to worse results as it did in my case with the RNN), then the effort was wasted. 
            </p>
            <p class="common-body-text">
               If I had started with a dense neural network, seen that it performed not great, moved onto the CNN, seen that it performed better, and the gone to the RNN, I wouldve realized early on that investing more time into the RNN wouldnt be worth it.
            </p>
            <p class="common-body-text">
               Its even likely that simpler models, like KNN, latent Dirichlet allocation, SVM, or other classical machine learning models, that used simple features like vocabulary counts would have performed better than all of the models I used in this exploration.
            </p>
            <h4>What Works In Practice Trumps Theory</h4>
            <p class="common-body-text">
               This lesson is almost in the same vein as the previous. Simply removing stopwords got a larger increase in the F1 score than implementing the attention mechanism. 
            </p>
            <p class="common-body-text">
               I fell into the trap of thinking that my RNN could simply learn that stopwords are meaningless, that my attention mechanism would be able to give stopwords a weight or zero, and that my RNN would be able to do whatever a CNN or dense neural network would be able to do by remembering everything in the sentence if I gave it a big enough hidden size. Maybe the thought about attention not being paid to stopwords is true. However, none of it worked in practice. 
            </p>
            <p class="common-body-text">
               If the CNN and dense neural network performs better, then who cares? They also have the benefit of being simpler to implement and debug. 
            </p>
            <p class="common-body-text">
               More importantly, if I waste all my time working on and debugging my RNN when there are simpler and more accurate models, then I care that my time was wasted. It was certainly worth it in the sense that there are some lessons that I just have to learn the hard way. The lessons will definitely stick then. 
            </p>
            <p class="common-body-text">
               This is somewhat similar to a lesson that Ive learned in the software industry from one of my managers:
            <blockquote> 
               If you dont code it, its not real.
            </blockquote>
            </p>
            <p class="common-body-text">
               If it works in theory but not in practice, then its not useful. 
            </p>
            <h4>Back to basics. Basics are the backbone.</h4>
            <p class="common-body-text">
               This may be a simple repeat of the above lessons, but it might be worth phrasing in a different context. Simply learning deep-learning but not classical machine learning is insufficient. 
            </p>
            <p class="common-body-text">
               When I read the advice to <i><a href="https://www.reddit.com/r/MachineLearning/comments/5z8110/d_a_super_harsh_guide_to_machine_learning/">Now forget all of that [machine learning] and read the deep learning book.</a></i>, I naively believed it would be foolish to learn all these older machine learning methods if Im not going to use them or if theyre not nearly as performant as the latest neural network models. 
            </p>
            <p class="common-body-text">
               This was foolish for a few reasons that many have already shared elsewhere online. 
            </p>
            <p class="common-body-text">
               Classical machine learning methods tend to be more interpretable (and easier to debug and to analyze).
            </p>
            <p class="common-body-text">
               Many techniques taught in earlier machine learning courses tend to apply to deep-learning as well. They arent mentioned often in deep learning tutorials since theyre often assumed to be known. Some examples include random oversampling and evaluation metrics. In this regard, I initially fell into the trap of thinking that my 99.999% accuracy score I initially got with my LSTM+Attention was due to the fact that my model was so great. It was because my model was great at being an uninformative classifier. If I had taken the time to learn all of these classical machine learning concepts, Idve known what an uninformative classifier was and how that might be a problem. 
            </p>
            <h4>Listen to your <a href="https://en.wikipedia.org/wiki/Kahuna">Kahunas</a>.</h4>
            <p class="common-body-text">
               This is similar to the previous lessons in that there are many places to learn useful things. One is from classical machine learning methods or other foundations. Another is from people whove been working the field for a long time or are influential in the field. 
            </p>
            <p class="common-body-text">
               There are many lessons I couldve learned from Andrej Karpathy if I deeply ingested his advice rather than shallowly glanced at it. 
            </p>
            <p class="common-body-text">
               He often advises to look at the data and become one with it. The neural network abstraction is <a href="https://www.joelonsoftware.com/2002/11/11/the-law-of-leaky-abstractions/">leaky</a>. It's not advisable to simply dump a neural network on top of arbitrary data (e.g. like I tried to dump an RNN on top of this text dataset) and expect things to magically work. It is vital to look at the data, learn what can be learned from it (this is the biggest part), and then see if the neural network model learn the same. 
            </p>
            <p class="common-body-text">
               I wasted a lot of time tuning my RNN before I even glanced at the data. Had I looked at the data, I wouldve realized how few examples some topics had. Some topics had 1 example. Some had 3. Many topics didnt have enough to have at least 1 example show up in each of the training/validation/testing splits. If I had realized this early on, I wouldve had a better idea of where to focus my efforts instead of spinning my wheels trying to increase an extremely low F1 sore caused by insufficient data. 
            </p>
            <p class="common-body-text">
               Looking at the data also wouldve helped me eliminate tons of unknown words. There were many numbers in the data. Knowledge of classical NLP methods wouldve helped me know to address this early on, but I didnt bother to learn those because I figured attention and the RNNs memory would have addressed this. Though the elimination of OOV words didnt increase performance, the excessive OOV words did cause the vocabulary word limit hyperparameter to have less meaning since the choice didnt accurately reflect how many meaningful words were being learned by the embedding layer. 
            </p>
            <p class="common-body-text">
               A similar problem occurred with <code>        \n</code> being a different token than <code>\n</code> Both of these were very common in the data and pushed out a lot of potentially meaningful words from being learned by the embedding layer. Looking at not only the raw data, but the pre-processed and tokenized data as well wouldve saved me some pain. 
            </p>
            <h3>Software Engineering Lessons</h3>
            <h4>Know The Law of Leaky Abstractions / The Hidden Benefits of a Strong C/C++ Background</h4>
            <p class="common-body-text">
               The Law of Leaky Abstractions showed up not only in debugging the neural networks to reveal <a href="https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b">how important it was to know how backpropagation worked</a> but also in how I had to deal with CUDA memory leaks in PyTorch. Python is a high-level language that intends to abstract away the potentially painful memory management and debugging issues that come with C/C++ with garbage collection. PyTorch similarly abstracts away dealing with memory management on the GPU. However, CUDA memory management issues still pop up. The memory management issues I had were solvable, but they would not have been had I not had a strong theoretical understanding of the underlying principles behind garbage collection and memory management. In dealing with these problems, I actually became quite skilled at debugging memory management issues by inspecting Pythons garbage collector to find out which unused tensors were being retained. 
            </p>
            <p class="common-body-text">
               It would be nice if I didnt have to deal with these issues, 
            </p>
            <h4>Study Data Like A Data Scientist; Write Code Like A Software Engineer</h4>
            <p class="common-body-text">
               Data science tutorials often involve giant scripts of data processing and excessive and unnecessary use of global values. 
            </p>
            <p class="common-body-text">
               This form is great for conveying ideas concisely with few lines of code and avoiding the distracting and unnecessary visual overhead of class definitions and abstractions and all that, but when it comes to functionality and practicality, abstractions come in handy when trying out new features. 
            </p>
            <p class="common-body-text">
               Principled software development and abstraction (in particular, the use of abstract classes) saved me tons of time when trying different models. Once I had written my training loop, data loader, check pointing functionality, etc., I never had to deal with it again. I could simply write a new nn.Module and have a new text classifier. If I had written the training loop like all the notebooks that I see on Kaggle, Id have to have changed things all over the place in different places along the giant script. With my abstract class definition, I only had to implement one or two methods for each model I wanted to use. 
            </p>
            <h4>Effective Experiment Scheduling</h4>
            <p class="common-body-text">
               Most people never learn how to effectively complete side-projects. The largest contributing factors are insufficient effort and insufficient time availability. The time availability problem is made worse for deep-learning and machine learning projects by the fact that training and hyperparameter tuning take a long time. This can cause a huge problem for many people given the way that many people work on their side projects (in contiguous blocks allocated on weekends). Deep-learning side-project productivity can be increased by intelligently scheduling the hyperparameter tuning and training. Schedule those to happen overnight <i>and</i> while at work. This will allow us to get results more frequently and more often. It additionally helps to put some minor amounts of effort in the morning before work to review the overnight results. This allows the testing if new ideas throughout the day while were busy at work. This will also increase the frequency that we work on our side project, which is beneficial because it increases the total time we look at our problem with fresh eyes. 
            </p>
            <h4>Learn Python Libraries</h4>
            <p class="common-body-text">
               Its extremely hard to know what someone means when they say that theyre a Python developer since someone can start writing and developing Python projects after at most an hour of tutorials. A strong Python developer can be noted by how well the developer can use Pythons most useful feature, which is the excessive number of both native and community-driven libraries. There are many useful utilities that can be found in Pythons native libraries that made writing a lot of code easier and faster since they let me write my code in a very concise way without having to write several of my own utilities. The libraries that proved extremely useful in this project (and that I believe everyone who boasts the title Python developer should know before making such claims) are <a href="https://docs.python.org/3/library/gc.html">gc</a>, <a href="https://docs.python.org/3/library/inspect.html">inspect</a>, <a href="https://docs.python.org/3/library/traceback.html">traceback</a>,  <a href="https://docs.python.org/3/library/contextlib.html">contextlib</a>, <a href="https://docs.python.org/3/library/functools.html">functools</a>, <a href="https://docs.python.org/3/library/itertools.html">itertools</a>, <a href="https://docs.python.org/3/library/multiprocessing.html">multiprocessing</a>, <a href="https://docs.python.org/3/library/collections.html">collections</a>. and <a href="https://docs.python.org/3/library/pdb.html">pdb</a>. The multiprocessing library was extremely useful for getting quick and easy speed ups in the data pre-processing pipeline to increase productivity. Other useful but non-vital libraries that were useful were <a href="https://github.com/tqdm/tqdm">tqdm</a>, <a href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/">bs4</a>, and <a href="https://pypi.org/project/colorama/">colorama</a>.
            </p>
            <h3>Last Words</h3>
            <p class="common-body-text">
               Theres a lot of wisdom to be gained by heeding the advice of others. <a href="https://cs.stanford.edu/people/karpathy/">Andrej Karpathy</a> and <a href="https://www.joelonsoftware.com/">Joel Spolsky</a> are two particularly wise wisemen. 
            </p>
            <p class="common-body-text">
               There are also many lessons that I shouldve learned from them that I wontve known I shouldve learned until I do things the hard way and go down the path of suffering. 
            </p>
            <p class="common-body-text">
               Luckily for my learning, there are many options that I didnt explore here and much more work to be done. Its not clear how to determine the effectiveness of these options without simply trying them. A meta-lesson learned is that it would be nice to learn how to estimate how promising certain routes are. This comes with practice and more projects. Look forward to more projects to come.
            </p>
	    </br>
	    </br>
	    </br>
         </div>
      </main>
      <footer class="globalFooter withCards">
         <section class="globalFooterCards" style="position: relative">
            <div style="opacity:0.1;">
               <div id="stripes" style="background: blue;-webkit-transform: skewY(0deg);transform: skewY(0deg);"></div>
            </div>
            <div class="container-xl">
               <a target="_blank" class="globalFooterCard" href="https://github.com/paul-tqh-nguyen" style="text-decoration: none;">
                  <img src="./front_end/github.svg" height="100" width="100"  style="padding-left: 60px;"/>
                  <center>
                     <p class="common-body-text" style="text-align: center; font-weight: bold;">Interested In My Work?</p>
                     </br>
                     <h2 class="common-UppercaseText">See my projects on GitHub.</h2>
                  </center>
               </a>
               <a target="_blank" class="common-Link globalFooterCard card-connect" href="https://paul-tqh-nguyen.github.io/about/" style="text-decoration: none;">
                  <img src="./front_end/website.svg" height="100" width="100"  style="padding-left: 60px;"/>
                  <center>
                     <p class="common-body-text" style="text-align: center; font-weight: bold;">Want to learn more about me?</p>
                     </br>
                     <h2 class="common-UppercaseText">Visit my website.</h2>
                  </center>
               </a>
            </div>
         </section>
      </footer>
   </body>
</html>
