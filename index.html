<!DOCTYPE html>
<html>
   <head>
      <title>Multi-Label Topic Classification on Reuters Newswire Articles</title>
      <link rel="stylesheet" href="./front_end/index.css">
   </head>
   <body>
      <div id="introduction">
         <center>
            <h1>Multi-Label Topic Classification on Reuters Newswire Articles</h1>
         </center>
         <p class="common-body-text">
            This document covers how well 3 different neural network architectures perform multi-label text classification on <a href="http://kdd.ics.uci.edu/databases/reuters21578/reuters21578.html">Reuters-21578 Text Categorization Collection</a>.
         </p>
         <p class="common-body-text">
	   Three neural network models were developed. 
         </p>
         <p class="common-body-text">Their architectures and performances are shown below:
	   <table>
	     <tr>
	       <th>Architecture</th>
	       <th>F1 Score</th>
	     </tr>
	     <tr>
               <td>CNN</td>
               <td>0.7655879</td>
	     </tr>
             <tr>
               <td>Dense Neural Network</td>
               <td>0.6601627</td>
             </tr>
             <tr>
               <td>RNN (LSTM+Attention)</td>
               <td>0.1065762</td>
             </tr>
	   </table>
         </p>
         <p class="common-body-text">
         </p>
         <p class="common-body-text">
            The code used in this document can be found <a href="https://github.com/paul-tqh-nguyen/reuters_topic_labelling">here</a>.
         </p>
         <p class="common-body-text">
           Table of Contents:
           <ul>
             <li><a href="#problem-description">Data Description & Problem Overview</a></li>
             <li><a href="#methods-employed">Methods Employed & Performance Evaluation</a></li>
             <li><a href="#implementation-details">Implementation Details & Code Demonstration</a></li>
             <li><a href="#conclusion">Conclusion & Lessons Learned</a></li>
           </ul>
         </p>
      </div>
      <main>
	<div>
	  <div id="top-spacer"></div>
	  <script>if (["#problem-description","#methods-employed","#implementation-details","#conclusion"].includes(window.location.hash)) {document.getElementById("top-spacer").style.height = "20vh";}</script>
          <div class="topic-section">
            <center id="problem-description">
              <h2 class="common-UppercaseTitle">
                <svg class="section-icon">
                  <circle fill="#eeb5f7" cx="33" cy="33" r="33">
                     </circle>
                     <path d="M15.7 45.3c-.7-2-.7-3.3-.7-8v-8.7c0-4.6 0-6 .7-8 .8-2.2 2.7-4 5-5 2-.6 3.3-.6 8-.6h8.7c4.6 0 6 0 8 .7 2.2.8 4 2.7 5 5 .6 2 .6 3.3.6 8v8.7c0 4.6 0 6-.7 8-.8 2.2-2.7 4-5 5-2 .6-3.3.6-8 .6h-8.7c-4.6 0-6 0-8-.7-2.2-.8-4-2.7-5-5z" fill="#beb5eb"></path>
                     <g>
                        <rect fill="#8e80ff" x="23" y="27" width="20" height="2" rx="1"></rect>
                        <circle fill="#8e80ff" cx="27.000185427800893" cy="28" r="4"></circle>
                     </g>
                     <g>
                        <rect fill="#8e80ff" x="23" y="37" width="20" height="2" rx="1"></rect>
                        <circle fill="#8e80ff" cx="27.0000000285928" cy="38" r="4"></circle>
                     </g>
                  </svg>
                  <span>
                  Data Description & Problem Overview
                  </span>
               </h2>
            </center>
            <h3>Original Dataset Overview</h3>
            <p class="common-body-text">
               We are looking at the <a href="http://kdd.ics.uci.edu/databases/reuters21578/reuters21578.html">Reuters-21578 Text Categorization Collection</a>.
            </p>
            <p class="common-body-text">
               A copy of the data that can be found in our <a href="https://github.com/paul-tqh-nguyen/reuters_topic_labelling/tree/master/data">repository</a>.
            </p>
            <p class="common-body-text">
               The dataset's provided <a href="https://github.com/paul-tqh-nguyen/reuters_topic_labelling/blob/master/data/README.txt">README</a> gives a good summary of what is provided. We'll provide a brief recap of the most pertinent content here:
            <ul>
               <li>The data is provided in <a href="https://en.wikipedia.org/wiki/Standard_Generalized_Markup_Language">SGML format</a>.</li>
               <li>The data contains 21,578 Reuters Newswire articles from 1987.</li>
               <li>There are 21 <code>.sgm</code> files with 1000 elements each. The 22nd and last <code>.sgm</code> file contains 578 elements. We'll examine those elements later in this section.</li>
               <li>They also provide a list of exchanges, orgs, people, places, and topics. All of this can be extracted directly form the <code>.sgm</code> files.</li>
            </ul>
            </p>
            <p class="common-body-text">
               Here's a brief look at some of the <code>.sgm</code> content:
            <pre>
		<code>
pnguyen@pnguyenmachine:/tmp/reuters_topic_labelling/data$ head -n 30 reut2-021.sgm
&lt;!DOCTYPE lewis SYSTEM &quot;lewis.dtd&quot;&gt;
&lt;REUTERS TOPICS=&quot;NO&quot; LEWISSPLIT=&quot;TEST&quot; CGISPLIT=&quot;TRAINING-SET&quot; OLDID=&quot;20436&quot; NEWID=&quot;21001&quot;&gt;
&lt;DATE&gt;19-OCT-1987 15:37:46.03&lt;/DATE&gt;
&lt;TOPICS&gt;&lt;/TOPICS&gt;
&lt;PLACES&gt;&lt;/PLACES&gt;
&lt;PEOPLE&gt;&lt;/PEOPLE&gt;
&lt;ORGS&gt;&lt;/ORGS&gt;
&lt;EXCHANGES&gt;&lt;/EXCHANGES&gt;
&lt;COMPANIES&gt;&lt;/COMPANIES&gt;
&lt;UNKNOWN&gt; 
&amp;#5;&amp;#5;&amp;#5;F 
&amp;#22;&amp;#22;&amp;#1;f2882&amp;#31;reute
f f BC-CITYFED-FINANCI   10-19 0013&lt;/UNKNOWN&gt;
&lt;TEXT TYPE=&quot;BRIEF&quot;&gt;&amp;#2;
******&lt;TITLE&gt;CITYFED FINANCIAL CORP SAYS IT CUT QTRLY DIVIDEND TO ONE CENT FROM 10 CTS/SHR
&lt;/TITLE&gt;Blah blah blah.
&amp;#3;

&lt;/TEXT&gt;
&lt;/REUTERS&gt;
&lt;REUTERS TOPICS=&quot;YES&quot; LEWISSPLIT=&quot;TEST&quot; CGISPLIT=&quot;TRAINING-SET&quot; OLDID=&quot;20435&quot; NEWID=&quot;21002&quot;&gt;
&lt;DATE&gt;19-OCT-1987 15:35:53.55&lt;/DATE&gt;
&lt;TOPICS&gt;&lt;D&gt;crude&lt;/D&gt;&lt;D&gt;ship&lt;/D&gt;&lt;/TOPICS&gt;
&lt;PLACES&gt;&lt;D&gt;bahrain&lt;/D&gt;&lt;D&gt;iran&lt;/D&gt;&lt;D&gt;usa&lt;/D&gt;&lt;/PLACES&gt;
&lt;PEOPLE&gt;&lt;/PEOPLE&gt;
&lt;ORGS&gt;&lt;/ORGS&gt;
&lt;EXCHANGES&gt;&lt;/EXCHANGES&gt;
&lt;COMPANIES&gt;&lt;/COMPANIES&gt;
&lt;UNKNOWN&gt; 
&amp;#5;&amp;#5;&amp;#5;Y 
pnguyen@pnguyenmachine:/tmp/reuters_topic_labelling/data$ 
		</code>
	      </pre>
            </p>
            <p class="common-body-text">
               Note the presence of <code>"Blah blah blah."</code> in the <code>.sgm</code> file. This is discussed in the <a href="https://github.com/paul-tqh-nguyen/reuters_topic_labelling/blob/master/data/README.txt">README</a>. These strings are placed where there was missing content. This type of content among others are handled during our data pre-processing.
            </p>
            <p class="common-body-text">
               Each of the articles contained in this dataset were encapsulated in the <code>.sgm</code> files as a <code>&lt;REUTERS&gt;</code> element.
            </p>
            <p class="common-body-text">
               Each <code>&lt;REUTERS&gt;</code> element could contain the following:
            </p>
            <p class="common-body-text">
               Not all of the <code>&lt;REUTERS&gt;</code> elements contained elements for each of the tags described above. There was sometimes not present data (e.g. some articles didn't mention any significant individual people, so there was nothing listed in the <code>&lt;PEOPLE&gt;</code> tag or the <code>&lt;PEOPLE&gt;</code> was entirely not present) and missing data (very often noted by the <code>"Blah blah blah."</code> string).
            </p>
            <h3>Dataset Pre-Processing Broad Overview</h3>
            <p class="common-body-text">
               This section will broadly go over our pre-processing pipeline.
            </p>
            <p class="common-body-text">
	      We used <a href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/">Beautiful Soup</a> to parse the <code>.sgm</code> files.
            </p>
            <p class="common-body-text">
               Our pre-processing pipeline generates two files, <a href="https://github.com/paul-tqh-nguyen/reuters_topic_labelling/blob/master/preprocessed_data/all_extracted_data.csv"><code>all_extracted_data.csv</code></a> and <a href="https://github.com/paul-tqh-nguyen/reuters_topic_labelling/blob/master/preprocessed_data/topics_data.csv"><code>topics_data.csv</code></a>.
            </p>
            <p class="common-body-text">
               <a href="https://github.com/paul-tqh-nguyen/reuters_topic_labelling/blob/master/preprocessed_data/all_extracted_data.csv"><code>all_extracted_data.csv</code></a> contains all the data present in the <code>.sgm</code> files in a <code>.csv</code> format. A row is generated for each article, i.e. <code>&lt;REUTERS&gt;</code> element, that has sufficient data (some <code>&lt;REUTERS&gt;</code> elements are corrupt or empty). 
            </p>
            <p class="common-body-text">
               <a href="https://github.com/paul-tqh-nguyen/reuters_topic_labelling/blob/master/preprocessed_data/topics_data.csv"><code>topics_data.csv</code></a> contains the <a href="https://en.wikipedia.org/wiki/Database_normalization">normalized</a> <code>&lt;TOPICS&gt;</code> data along with the corresponding article. 
            </p>
            <p class="common-body-text">
               Our dataset pre-processing pipeline is:
            <ol>
               <li>We extract all of the <code>&lt;REUTERS&gt;</code> elements from the 22 <code>.sgm</code> files.</li>
               <li>
                  From each <code>&lt;REUTERS&gt;</code> article, we extract:
                  <ul style="padding-top: 20px">
                     <li style="margin-left: -10vw; font-size: 20px; margin-bottom: 5px;">the article date via <code>&lt;DATE&gt;</code>.</li>
                     <li style="margin-left: -10vw; font-size: 20px; margin-bottom: 5px;">the list of a topics (a string with comma separated values, e.g. <code>"crude, ship, oil"</code>) via <code>&lt;TOPICS&gt;</code>.</li>
                     <li style="margin-left: -10vw; font-size: 20px; margin-bottom: 5px;">the list of places mentioned in the article via <code>&lt;PLACES&gt;</code>.</li>
                     <li style="margin-left: -10vw; font-size: 20px; margin-bottom: 5px;">the list of people mentioned in the article via <code>&lt;PEOPLE&gt;</code>.</li>
                     <li style="margin-left: -10vw; font-size: 20px; margin-bottom: 5px;">the list of organizations mentioned in the article via <code>&lt;ORGS&gt;</code>.</li>
                     <li style="margin-left: -10vw; font-size: 20px; margin-bottom: 5px;">the list of companies mentioned in the article via <code>&lt;COMPANIES&gt;</code>.</li>
                     <li style="margin-left: -10vw; font-size: 20px; margin-bottom: 5px;">the article title via <code>&lt;TITLE&gt;</code>.</li>
                     <li style="margin-left: -10vw; font-size: 20px; margin-bottom: 5px;">the article <a href="https://en.wikipedia.org/wiki/Dateline">dateline</a> via <code>&lt;DATELINE&gt;</code>.</li>
                     <li style="margin-left: -10vw; font-size: 20px; margin-bottom: 5px;">the raw original article text via <code>&lt;TEXT&gt;</code>.</li>
                     <li style="margin-left: -10vw; font-size: 20px; margin-bottom: 5px;">the preprocessed article text (we'll cover this in detail momentarily).</li>
                     <li style="margin-left: -10vw; font-size: 20px; margin-bottom: 5px;">whatever arbitrary content is in the <code>&lt;UNKNOWN&gt;</code> element.</li>
                  </ul>
               </li>
               <li>The above data (along with the ordinal position of the <code>&lt;REUTERS&gt;</code> element within the <code>.sgm</code> file and the name of the relevant <code>.sgm</code> file) is stored in <a href="https://github.com/paul-tqh-nguyen/reuters_topic_labelling/blob/master/preprocessed_data/all_extracted_data.csv"><code>all_extracted_data.csv</code></a>. <code>&lt;REUTERS&gt;</code> elements with an empty or corrupt <code>&lt;TEXT&gt;</code> sub-element are not included.</li>
               <li>The topics data is then <a href="https://en.wikipedia.org/wiki/Database_normalization">normalized</a> to create <a href="https://github.com/paul-tqh-nguyen/reuters_topic_labelling/blob/master/preprocessed_data/topics_data.csv"><code>topics_data.csv</code></a>. We removed topics that didn't have a sufficient number of associated articles. We require each topic to have at least 200 articles. Most models are incapable of learning to associate topics given an insufficient number of samples.</li>
            </ol>
            </p>
            <p class="common-body-text">
               Our article text pre-processining pipeline is (in order):
            <ol>
               <li>Lower case the string.</li>
               <li>Fix common typos, e.g. "<code>"s</code>" was often used in place of "<code>'s</code>" in the raw text.</li>
               <li><a href="https://en.wikipedia.org/wiki/Ellipsis">Ellipsis</a> collapsing (i.e. replacing all text matching the reguular expression <code>"\.\.\.\.+"</code> with the string <code>"..."</code>).</li>
               <li>Replacing numbers with a special <code>NUMBER</code> token (in order to decrease the unknown token count).</li>
               <li><a href="https://en.wikipedia.org/wiki/Contraction_(grammar)">Contraction</a> expansion.</li>
               <li>White space stripping.</li>
               <li><a href="https://en.wikipedia.org/wiki/Stop_words">Stopword</a> removal.</li>
               <li><a href="https://en.wikipedia.org/wiki/Lemmatisation">Lemmatization</a>.</li>
            </ol>
            </p>
            <p class="common-body-text">
              Here's a sample of <a href="https://github.com/paul-tqh-nguyen/reuters_topic_labelling/blob/master/preprocessed_data/all_extracted_data.csv"><code>all_extracted_data.csv</code></a> via <a href="https://pandas.pydata.org/">Pandas</a>:
            <pre>
		<code>
&gt;&gt;&gt; df = pd.read_csv('all_extracted_data.csv')
&gt;&gt;&gt; df.head
&lt;bound method NDFrame.head of                           date  topics_raw_string                        places  ...                                               text                  file reuter_element_position
0       2-JUN-1987 10:04:07.81            ['cpi']                    ['brazil']  ...  president jose sarney today declare &quot;a war wit...  ./data/reut2-018.sgm                       0
1       2-JUN-1987 10:05:19.67                 []                       ['usa']  ...  sale new, single-family home rose NUMBER . &lt;...  ./data/reut2-018.sgm                       2
2       2-JUN-1987 10:06:06.42                 []                        ['uk']  ...  brixton estate plc establish sterling commerci...  ./data/reut2-018.sgm                       3
3       2-JUN-1987 10:06:53.99                 []                       ['usa']  ...  president reagan say paul volcker decline serv...  ./data/reut2-018.sgm                       4
4       2-JUN-1987 10:11:04.12                 []                   ['belgium']  ...  belgium's public expenditure deficit fell shar...  ./data/reut2-018.sgm                       5
...                        ...                ...                           ...  ...                                                ...                   ...                     ...
19037  19-OCT-1987 21:22:52.77       ['money-fx']              ['japan', 'usa']  ...  japan's finance minister kiichi miyazawa say r...  ./data/reut2-019.sgm                     992
19038  19-OCT-1987 21:58:03.49                 []               ['philippines']  ...  explosive device blow philippine congress buil...  ./data/reut2-019.sgm                     994
19039  19-OCT-1987 22:08:52.87  ['crude', 'ship']      ['japan', 'usa', 'iran']  ...  japan say understood u.s. attack iranian oil p...  ./data/reut2-019.sgm                     995
19040  19-OCT-1987 23:05:03.63          ['crude']  ['venezuela', 'usa', 'iran']  ...  world oil price would remain stable despite u....  ./data/reut2-019.sgm                     997
19041  19-OCT-1987 23:30:37.76                 []                     ['japan']  ...  canon inc &lt;cann.t&gt; say stop make japanese lang...  ./data/reut2-019.sgm                     998

[19042 rows x 14 columns]&gt;
&gt;&gt;&gt; df.columns
Index(['date', 'topics_raw_string', 'places', 'people', 'orgs', 'exchanges',
       'companies', 'unknown', 'text_title', 'text_dateline', 'raw_text',
       'text', 'file', 'reuter_element_position'],
      dtype='object')
&gt;&gt;&gt; {print(e) for e in {column:df[column][0] for column in df.columns}.items()}
('date', '2-JUN-1987 10:04:07.81')
('topics_raw_string', &quot;['cpi']&quot;)
('places', &quot;['brazil']&quot;)
('people', &quot;['sarney']&quot;)
('orgs', '[]')
('exchanges', '[]')
('companies', '[]')
('unknown', '[&quot; \\n\\x05\\x05\\x05C\\n\\x16\\x16\\x01f1027\\x1freute\\nu f BC-BRAZIL\'S-SARNEY-DECLA   06-02 0092&quot;]')
('text_title', &quot;BRAZIL'S SARNEY RENEWS CALL FOR WAR ON INFLATION&quot;)
('text_dateline', '    BRASILIA, June 2 - ')
('raw_text', 'President Jose Sarney today declared &quot;a\nwar without quarter&quot; on inflation and said the government would\nwatch every cent of public expenditure.\n    Sarney, addressing his cabinet live on television, also\nreiterated that he intended to remain in power for five years,\nuntil 1990. There has been a long-running political debate\nabout how long his mandate should be.\n    Brazil is currently suffering from the worst inflation of\nits history. In April monthly inflation reached 21 pct.\n Reuter\n\x03')
('text', 'president jose sarney today declare &quot;a war without quarter&quot; inflation say government would watch every cent public expenditure. sarney, address cabinet live television, also reiterate intend remain power five years, &lt;NUMBER&gt; . long-running political debate long mandate be. brazil currently suffer bad inflation history. april monthly inflation reach &lt;NUMBER&gt; pct. reuter')
('file', './data/reut2-018.sgm')
('reuter_element_position', 0)
{None}
&gt;&gt;&gt; 
		</code>
	      </pre>
            </p>
            <p class="common-body-text">
               We were able to extract 19,042 rows with sufficient and non-corrupt article text data. 
            </p>
            <p class="common-body-text">
               There are 14 columns in <a href="https://github.com/paul-tqh-nguyen/reuters_topic_labelling/blob/master/preprocessed_data/all_extracted_data.csv"><code>all_extracted_data.csv</code></a> that correspond to what was discussed in our dataset pre-processing pipeline overview above. 
            </p>
            <p class="common-body-text">
               Note that some columns contain strings that denote via comma-separation multiple values. The data in these columns are <a href="https://en.wikipedia.org/wiki/Database_normalization">unnormalized</a>. The topics are stored in such a column. <a href="https://github.com/paul-tqh-nguyen/reuters_topic_labelling/blob/master/preprocessed_data/topics_data.csv"><code>topics_data.csv</code></a> contains the <a href="https://en.wikipedia.org/wiki/Database_normalization">normalized</a> topic data.
            </p>
            <p class="common-body-text">
               Here's a sample of <a href="https://github.com/paul-tqh-nguyen/reuters_topic_labelling/blob/master/preprocessed_data/topics_data.csv"><code>topics_data.csv</code></a> via <a href="https://pandas.pydata.org/">Pandas</a>:
            <pre>
		<code>
&gt;&gt;&gt; df = pd.read_csv('topics_data.csv')
&gt;&gt;&gt; df.columns
Index(['text_dateline', 'text_title', 'raw_text', 'text',
       'reuter_element_position', 'date', 'file', 'money-fx', 'acq', 'grain',
       'earn', 'corn', 'interest', 'trade', 'crude', 'ship', 'wheat'],
      dtype='object')
&gt;&gt;&gt; {print(e) for e in {column:df[column][0] for column in df.columns}.items()}
('reuter_element_position', 11)
('raw_text', 'Newly-nominated Federal Reserve Board\nchairman Alan Greenspan said there was evidence the dollar\nfinally had bottomed out.\n    In a White House briefing Greenspan was asked by reporters\nif he thought the dollar had bottomed out.\n    &quot;There certainly is evidence in that direction,&quot; he replied.\n Reuter\n\x03')
('text', 'newly-nominated federal reserve board chairman alan greenspan say evidence dollar finally bottom out. white house brief greenspan ask reporter think dollar bottom out. &quot;there certainly evidence direction,&quot; replied. reuter')
('date', '2-JUN-1987 10:21:40.24')
('file', './data/reut2-018.sgm')
('text_title', 'GREENSPAN SEES EVIDENCE DOLLAR FALL OVER')
('text_dateline', '    WASHINGTON, June 2 - ')
('money-fx', True)
('acq', nan)
('grain', nan)
('earn', nan)
('corn', nan)
('interest', nan)
('trade', nan)
('crude', nan)
('ship', nan)
('wheat', nan)
{None}
&gt;&gt;&gt; 
		</code>
	      </pre>
            </p>
            <p class="common-body-text">
               We were only able to extract 8,599 rows with sufficient data for <a href="https://github.com/paul-tqh-nguyen/reuters_topic_labelling/blob/master/preprocessed_data/topics_data.csv"><code>topics_data.csv</code></a>. This is due to the fact that many of the articles had topics with an insufficient article count (we used a threshold of 200). An insufficient article count would make the development of a well-performing model challenging if not impossible. 
            </p>
            <p class="common-body-text">
	      Most of the data in the original dataset was sparse w.r.t. topic content. There were 10 topics that had a sufficient number of articles.
            </p>
            <h3>Problem Selection</h3>
            <p class="common-body-text">
               There are many problems we could attempt to solve, e.g. predicting the article title given the article text, extracting the names of the mentioned people or organizations from the text, or generating the dateline given the article text. 
            </p>
            <p class="common-body-text">
               The problem we chose for this dataset was multi-label topic classification. Our goal was to develop a model to predict the possibly many topics associated with each article. This is a <a href="https://en.wikipedia.org/wiki/Multi-label_classification">multi-label classification problem</a>. 
            </p>
            <p class="common-body-text">
               Here's an example of our desired product:
            </p>
            <div id="desired-interface-table">
               <table>
                  <tr>
                     <td style="text-align: left">
                       <pre>
			 <code>
The Panamanian bulk carrier Juvena is
still aground outside Tartous, Syria, despite discharging 6,400
tons of its 39,000-ton cargo of wheat, and water has entered
the engine-room due to a crack in the vessel bottom, Lloyds
Shipping Intelligence Service said.
    The Juvena, 53,351 tonnes dw, ran aground outside Tartous
port basin breakwater on February 25 in heavy weather and rough
seas.
 Reuter
			</code>
		      </pre>
                     </td>
                     <td style="text-align: div;font-size:10vw; color: darkviolet;">&#8649;</td>
                     <td style="text-align: left">
                        <pre>
			<code>
     acq <strong style="color: grey;">False</strong>
    corn <strong style="color: grey;">False</strong>
   crude <strong style="color: grey;">False</strong>
    earn <strong style="color: grey;">False</strong>
   grain <strong style="color: purple;">True</strong>
interest <strong style="color: grey;">False</strong>
money-fx <strong style="color: grey;">False</strong>
    ship <strong style="color: purple;">True</strong>
   trade <strong style="color: grey;">False</strong>
   wheat <strong style="color: purple;">True</strong>
			</code>
		      </pre>
                     </td>
                  </tr>
                  <tr>
                     <td style="text-align: left">
                        <pre>
			<code>
Farmers enrolled over 6.5 mln acres
of program crops in the latest conservation reserve program
signup and around four mln acres of non-program crops,
Agriculture Department conservation specialists said.
    Soybean acreage amounted to less than two mln acres of the
non-program crop acreage enrolled, a USDA analyst said. Heavy
enrollment of non-base acreage in wheat states, of which a big
percentage would be fallow and non-soybean land, accounted for
a large portion of the non-program acreage, the analyst said.
    Wheat and corn acreage comprised slightly over 40 pct of
the total 10,572,402 acres accepted into the ten-year program.
    USDA analysts gave the following enrollment breakdown:
    -- wheat  2,615,140 acres
    -- corn   1,894,764 acres
    -- barley   705,888 acres
    -- sorghum  585,552 acres
    -- cotton   417,893 acres
    -- rice       2,035 acres
    -- peanuts      611 acres
    -- tobacco      285 acres
    -- total program crops  6,512,700 acres
    -- total nonprogram     4,059,702 acres
    -- total enrollment    10,572,402 acres
    USDA analysts are currently working on a complete state
breakdown of crop acreage enrollment and should have it ready
for publication later this week, they said.
 Reuter
			</code>
		      </pre>
                     </td>
                     <td style="text-align: div;font-size:10vw; color: darkviolet;">&#8649;</td>
                     <td style="text-align: left">
                        <pre>
			<code>
     acq <strong style="color: grey;">False</strong>
    corn <strong style="color: purple;">True</strong>
   crude <strong style="color: grey;">False</strong>
    earn <strong style="color: grey;">False</strong>
   grain <strong style="color: purple;">True</strong>
interest <strong style="color: grey;">False</strong>
money-fx <strong style="color: grey;">False</strong>
    ship <strong style="color: grey;">False</strong>
   trade <strong style="color: grey;">False</strong>
   wheat <strong style="color: purple;">True</strong>
			</code>
		      </pre>
                     </td>
                  </tr>
                  <tr>
                     <td style="text-align: left">
                        <pre>
			<code>

Sen. David Pryor, D-Ark., said
he was considering amending the Senate Finance Committee's
trade bill with a provision to require a marketing loan for
soybeans, corn and wheat.
    Pryor told the Futures Industry Association that there was
great reluctance among members of the Senate Agriculture
Committee to reopen the 1985 farm bill, and that a marketing
loan might have a better chance in the Finance panel.
    The Arkansas senator said the marketing loan -- which in
effect allows producers to pay back their crop loans at the
world price -- had led to a 300 pct increase in U.S. cotton
exports in 14 months and a 72 pct increase in rice exports.
    Pryor serves on both the Senate Finance and Agriculture
Committees.
 Reuter
			</code>
		      </pre>
                     </td>
                     <td style="text-align: div;font-size:10vw; color: darkviolet;">&#8649;</td>
                     <td style="text-align: left">
                        <pre>
			<code>
     acq <strong style="color: grey;">False</strong>
    corn <strong style="color: purple;">True</strong>
   crude <strong style="color: grey;">False</strong>
    earn <strong style="color: grey;">False</strong>
   grain <strong style="color: purple;">True</strong>
interest <strong style="color: grey;">False</strong>
money-fx <strong style="color: grey;">False</strong>
    ship <strong style="color: grey;">False</strong>
   trade <strong style="color: purple;">True</strong>
   wheat <strong style="color: purple;">True</strong>
			</code>
		      </pre>
                     </td>
                  </tr>
               </table>
            </div>
            <p class="common-body-text">
               More details about the methods we used to solve this problem come in the <a href="#methods-employed">next section</a>.
            </p>
         </div>
         </div>
         <div id="methods-employed" class="topic-section">
            <center>
               <h2 class="common-UppercaseTitle">
                  <svg class="section-icon" >
                     <circle fill="#eeb5f7" cx="33" cy="33" r="33">
                     </circle>
                     <path d="M38.4 15l1-3h1l1.2 3c.2.2.5.2.7.3l2.2-2.5 1 .4-.2 3.3c.2 0 .3.2.5.4l3-1.5.7.7-1.4 3 .5.5h3.3l.4.8-2.5 2.2c0 .2 0 .5.2.7l3 1v1l-3 1.2-.3.8 2.5 2-.4 1-3.3-.2-.4.7 1.5 2.8-.7.7-3-1.4c0 .2-.4.4-.6.5l.2 3.3-1 .4-2-2.5c-.3 0-.6 0-1 .2l-1 3h-1l-1-3c-.2-.2-.5-.2-.8-.3l-2 2.5-1-.4.2-3.3-.7-.4-2.8 1.5-.7-.7 1.4-3c-.2 0-.4-.4-.5-.6l-3.3.2-.4-1 2.5-2c0-.3 0-.6-.2-1l-3-1v-1l3-1c.2-.2.2-.4.3-.7l-2.5-2.2.4-1 3.3.2c0-.2.2-.3.4-.5l-1.5-3 .7-.7 3 1.4.5-.5v-3.3l.8-.4 2.2 2.5s.5 0 .7-.2z" fill="#a784e0" transform="rotate(3.999359999999986 40 25)">
                     </path>
                     <circle fill="#eeb5f7" cx="40" cy="25" r="2">
                     </circle>
                     <path d="M21.6 26.8L19 25l-1.3 1 1.4 3c0 .2-.3.4-.5.6l-3-.8-1 1.4 2.4 2.3-.4.8-3.2.3-.3 1.6 3 1.4v.8l-3 1.4.4 1.6 3.2.3c0 .3.2.5.3.8l-2.4 2.3.8 1.4 3-.8.7.6-1.3 3 1.3 1 2.6-1.8c.3 0 .5.3.8.4l-.3 3.2 1.6.6 2-2.7c.2 0 .5 0 .7.2l1 3h1.5l1-3c0-.2.4-.2.7-.3l2 2.7 1.4-.6-.4-3.2c.3 0 .5-.3.8-.4L37 49l1.3-1-1.4-3c0-.2.3-.4.5-.6l3 .8 1-1.4-2.4-2.3.4-.8 3.2-.3.3-1.6-3-1.4v-.8l3-1.4-.4-1.6-3.2-.3c0-.3-.2-.5-.3-.8l2.4-2.3-.8-1.4-3 .8-.7-.6 1.3-3-1.3-1-2.6 1.8c-.3 0-.5-.3-.8-.4l.3-3.2-1.6-.6-2 2.7c-.2 0-.5 0-.7-.2l-1-3h-1.5l-1 3c0 .2-.4.2-.7.3l-2-2.7-1.4.6.4 3.2c-.3 0-.5.3-.8.4z" fill="#8e80ff" transform="rotate(-3.999359999999986 28 37)">
                     </path>
                     <circle fill="#eeb5f7" cx="28" cy="37" r="3">
                     </circle>
                  </svg>
                  <span>
                  Methods Employed & Performance Evaluation
                  </span>
               </h2>
            </center>
            <div style="opacity:0.05;"><div id="stripes" style="background: blueviolet;"></div></div>
            <h3>Evaluation Method</h3>
            <p class="common-body-text">
               We're attempting to handle a <a href="https://en.wikipedia.org/wiki/Multi-label_classification">multi-label text classification</a> problem.
            </p>
            <p class="common-body-text">
               Most articles in our dataset have only a few topics. If an article has a few topics, then there are few positive choices and many negative choices. Thus, we have a dataset imbalance. Because of this data imbalance we chose to use the <a href="https://en.wikipedia.org/wiki/F1_score">F1 metric</a> to evaluate the performance of our models. 
            </p>
            <h3>Model Results</h3>
            <p class="common-body-text">
               We implemented 3 neural net models, an RNN, a CNN, and a dense neural network. 
            </p>
            <p class="common-body-text">
               We performed random search over the hyperparameter space to determine the best possible score for each model.
            </p>
            <p class="common-body-text">
               We split dataset to allocate 50% of examples for training, 20% of examples for validation, and 30% of examples for testing.
            </p>
            <p class="common-body-text">
               Our CNN performed the best and got a test set F1 score of <code>0.7655879</code>.
            </p>
            <p class="common-body-text">
               Our dense neural network did not perform as well and got a test set F1 score of <code>0.6601627</code>.
            </p>
            <p class="common-body-text">
               Our RNN performed very poorly a test set F1 score of <code>0.1065762</code>.
            </p>
            <h3>Neural Network Architectures</h3>
            <p class="common-body-text">
               We'll go over the specifics of the neural network architectures used. 
            </p>
            <p class="common-body-text">
               All of our models used <a href="https://en.wikipedia.org/wiki/Word_embedding">pre-trained word embeddings</a>. Choice of pre-trained word embedding is one of the hyperparameters optimized via random search. The pre-trained word embedding options are shown below:
            <ul>
               <li><a href="https://github.com/hassyGo/charNgram2vec">100-dimensional charNgram2vec embeddings</a> (<a href="https://arxiv.org/abs/1611.01587">relevant research paper</a>).</li>
               <li><a href="https://fasttext.cc/docs/en/pretrained-vectors.html">300-dimensional fastText skip-gram embeddings trained on Simple English Wikipedia</a> (<a href="https://arxiv.org/abs/1607.04606">relevant research paper</a>).</li>
               <li><a href="https://fasttext.cc/docs/en/pretrained-vectors.html">300-dimensional fastText skip-gram embeddings trained on English Wikipedia</a> (<a href="https://arxiv.org/abs/1607.04606">relevant research paper</a>).</li>
               <li><a href="https://nlp.stanford.edu/projects/glove/">25-dimensional GloVe embeddings trained on Twitter</a> (<a href="https://nlp.stanford.edu/pubs/glove.pdf">relevant research paper</a>).</li>
               <li><a href="https://nlp.stanford.edu/projects/glove/">50-dimensional GloVe embeddings trained on Twitter</a> (<a href="https://nlp.stanford.edu/pubs/glove.pdf">relevant research paper</a>).</li>
               <li><a href="https://nlp.stanford.edu/projects/glove/">100-dimensional GloVe embeddings trained on Twitter</a> (<a href="https://nlp.stanford.edu/pubs/glove.pdf">relevant research paper</a>).</li>
               <li><a href="https://nlp.stanford.edu/projects/glove/">200-dimensional GloVe embeddings trained on Twitter</a> (<a href="https://nlp.stanford.edu/pubs/glove.pdf">relevant research paper</a>).</li>
               <li><a href="https://nlp.stanford.edu/projects/glove/">300-dimensional GloVe embeddings trained on CommonCrawl with 42 billion tokens</a> (<a href="https://nlp.stanford.edu/pubs/glove.pdf">relevant research paper</a>).</li>
               <li><a href="https://nlp.stanford.edu/projects/glove/">300-dimensional GloVe embeddings trained on CommonCrawl with 840 billion tokens</a> (<a href="https://nlp.stanford.edu/pubs/glove.pdf">relevant research paper</a>).</li>
               <li><a href="https://nlp.stanford.edu/projects/glove/">50-dimensional GloVe embeddings trained on English Wikipedia</a> (<a href="https://nlp.stanford.edu/pubs/glove.pdf">relevant research paper</a>).</li>
               <li><a href="https://nlp.stanford.edu/projects/glove/">100-dimensional GloVe embeddings trained on English Wikipedia</a> (<a href="https://nlp.stanford.edu/pubs/glove.pdf">relevant research paper</a>).</li>
               <li><a href="https://nlp.stanford.edu/projects/glove/">200-dimensional GloVe embeddings trained on English Wikipedia</a> (<a href="https://nlp.stanford.edu/pubs/glove.pdf">relevant research paper</a>).</li>
               <li><a href="https://nlp.stanford.edu/projects/glove/">300-dimensional GloVe embeddings trained on English Wikipedia</a> (<a href="https://nlp.stanford.edu/pubs/glove.pdf">relevant research paper</a>).</li>
            </ul>
            </p>
            <p class="common-body-text">
               Below is a description of our CNN's architecture:
            <ol>
               <li>The first layer was an embedding layer.</li>
               <li>The second layer was a set of several parallel convolutional layers that the sequence of beddings were passed through.</li>
               <li>The third layer was a pooling layer (the choice of max or average pooling was a hyperparameter) that pooled the results of each convolutional layer.</li>
               <li>The final layer was a fully connected layer that took the concatenation of all the pooling results.</li>
            </ol>
            </p>
            <p class="common-body-text">
               Below is a description of our RNN's architecture:
            <ol>
               <li>The first layer was an embedding layer.</li>
               <li>The second layer was a bi-directional LSTM encoding layer.</li>
               <li>The third layer was an attention layer using <a href="https://arxiv.org/abs/1703.03130">Zhouhan Lin's attention mechanism</a>.</li>
               <li>The final layer was a fully connected layer.</li>
            </ol>
            </p>
            <p class="common-body-text">
               Below is a description of our dense neural network's architecture:
            <ol>
               <li>Prior to going through any layers, our input sequence was truncated or padded to a max sequence length (a hyperparameter).</li>
               <li>The first layer was an embedding layer.</li>
               <li>The intermediate layers were all fully connected layers (the size of each layer and number of layers were hyperparameters).</li>
               <li>The final layer was a fully connected layer.</li>
            </ol>
            </p>
            <p class="common-body-text">
	      Some stats regarding our best CNN are:
	      <table id="hyperparameter-stats">
		<tr>
		  <th>Property</th>
		  <th>Value</th>
		</tr>
		<tr>
		  <td>Number of Epochs to Convergence</td>
		  <td>16</td>
		</tr>
		<tr>
		  <td>Parameter Count</td>
		  <td>6,259,190</td>
		</tr>
		<tr>
		  <td>Batch Size</td>
		  <td>64</td>
		</tr>
		<tr>
		  <td>Vocab Size</td>
		  <td>15,697</td>
		</tr>
		<tr>
		  <td>Pre-trained Word Embedding</td>
		  <td><a href="https://fasttext.cc/docs/en/pretrained-vectors.html">300D fastText English Wikipedia skip-gram</a></td>
		</tr>
		<tr>
		  <td>Convolution Layer Hidden Size</td>
		  <td>256</td>
		</tr>
		<tr>
		  <td>Parallel Convolution Layers' Kernel Sizes</td>
		  <td>[2, 3, 4, 5, 6]</td>
		</tr>
		<tr>
		  <td>Pooling Method</td>
		  <td>Max Pooling</td>
		</tr>
		<tr>
		  <td>Dropout Probability</td>
		  <td>0.5</td>
		</tr>
		<tr>
		  <td>Validation Loss</td>
		  <td>0.04981713553481713</td>
		</tr>
		<tr>
		  <td>Test F1</td>
		  <td>0.765587901627576</td>
		</tr>
		<tr>
		  <td>Test Loss</td>
		  <td>0.05267227948125866</td>
		</tr>
	      </table>
            </p>
            <p class="common-body-text">
               There is always room for more work and we could try several other different models (e.g. <a href="https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation">Latent Dirichlet allocation</a>) and methods as well (e.g. implementing <a href="https://arxiv.org/abs/1703.03130">Zhouhan Lin’s attention regularization</a>). The work presented here does not demonstrate an exhaustive search over all possible known methods and models. This is solely an evaluation of the methods described above.
            </p>
            <h3>Loss functions</h3>
            <p class="common-body-text">
               There are many ways to represent our problem for multi-label classification (some are listed <a href="https://en.wikipedia.org/wiki/Multi-label_classification#Problem_transformation_methods">here</a>). We explored two loss functions, sum of <a href="https://en.wikipedia.org/wiki/Cross_entropy">BCE</a> across all topics and <a href="https://towardsdatascience.com/the-unknown-benefits-of-using-a-soft-f1-loss-in-classification-systems-753902c0105d">Soft-F1</a>. 
            </p>
            <p class="common-body-text">
               We used both functions but found that neither seemed to lead to a better F1 score than the other.
            </p>
            <h3>Data Imbalance</h3>
            <p class="common-body-text">
               With binary classification problems, data imbalance can be solved easily through oversampling or undersampling as there are only two categories to balance. We sample until the two categories are the same size. For single-label multi-class classification, we can employ a similar method. 
            </p>
            <p class="common-body-text">
               With multi-label classification, the problem of determining which oversampling or undersampling is not clear. 
            </p>
            <p class="common-body-text">
               Our understanding is that there are two classes of methods for handling data imbalance, generating samples over a discrete space and generating samples over a continuous space. 
            </p>
            <p class="common-body-text">
               Example methods of generating samples over a discrete space might include:
            <ul>
               <li>Random oversampling/undersampling of existing data</li>
               <li>Generating data-augmented samples</li>
            </ul>
            </p>
            <p class="common-body-text">
               Example methods of generating samples over a continuous space might include:
            <ul>
               <li>SMOTE</li>
               <li>ADASYN</li>
            </ul>
            </p>
            <p class="common-body-text">
               For NLP, it seems that generating samples over a continuous space isn’t always applicable, and it is not obvious from the current literature that this would be effective in the NLP. One possible method of doing this might be using SMOTE (or some similar method) to generate synthetic sequences of word embeddings. Since it wasn't clear that this was a promising path, we did nto prioritize exploring this option. Results from this will come in the future.
            </p>
            <p class="common-body-text">
               For generating samples over a discrete space, data-augmentation is a known method that works (e.g. <a href="https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/52557">Kaggle competitors have augmented their data by translating to a different language and translating back to the original language</a>; people have also augmented text by using synonym replacement). This helps augment the data and give a richer input dataset, but the labels might still be unbalanced, especially for a multi-label dataset.
            </p>
            <p class="common-body-text">
               As far as we could tell, selecting which samples to oversample or undersample and by how much is not clear since some samples might decrease imbalance by incrementing one underrepresented label’s count but increase imbalance by incrementing an overrepresented label’s count.
            </p>
            <p class="common-body-text">
	      We'll below present our (possibly novel? Hopefully more easily findable) method of solving this problem.
            </p>
            <p class="common-body-text">
              This seems to be an <a href="https://en.wikipedia.org/wiki/Integer_programming">integer programming problem</a> (and <a href="https://en.wikipedia.org/wiki/NP-completeness">NP-complete</a>). We can approximately solve this by oversampling or undersampling datapoints such that the label histogram reflects a <a href="https://en.wikipedia.org/wiki/Uniform_distribution_(continuous)">uniform distribution</a>.
            </p>
            <p class="common-body-text">
              We can have a datapoint occur <code>N</code> times (<code>N<1</code> leads to undersampling the datapoint; <code>N>1</code> leads to oversampling the datapoint). Different choices in <code>N</code> for each datapoint will lead to different label distributions. 
            </p>
            <p class="common-body-text">
               We can calculate how different the label distribution is from a <a href="https://en.wikipedia.org/wiki/Uniform_distribution_(continuous)">uniform distribution</a> using the <a href="https://en.wikipedia.org/wiki/Chi-squared_test">Chi-Squared test</a>. 
            </p>
            <p class="common-body-text">
               If we constrain the choices for <code>N</code> to be an integer, this will be an <a href="https://en.wikipedia.org/wiki/NP-completeness">NP-complete</a> <a href="https://en.wikipedia.org/wiki/Integer_programming">integer programming problem</a>. If we let <code>N</code> be continuous, we can optimize this since the <a href="https://en.wikipedia.org/wiki/Chi-squared_test">Chi-Squared test</a> is differentiable. We can get an approximate solution by truncating the <code>N</code>.
            </p>
            <p class="common-body-text">
               Other differentiable objective functions can be chosen (some good ideas can be found in <a href="https://en.wikipedia.org/wiki/Exact_test">here</a>, <a href="https://en.wikipedia.org/wiki/Statistical_significance">here</a>, and <a href="https://en.wikipedia.org/wiki/Goodness_of_fit">here</a>). Exploring these options would be interesting future work. 
            </p>
            <p class="common-body-text">
              Though this was an interesting and fun idea to explore, we tried to oversample the dataset by varying amounts up to 3x the size of the original dataset while minimizing the label distributions difference from a uniform distribution. No noticeable improvement was found. 
            </p>
            <p class="common-body-text">
              We believe that this method might in general be useful for cases where data imbalance causes problems, but it might not be useful for this given dataset due to the small size where the data imbalance is not the dominating issue.
            </p>
            <h3>Data Augmentation</h3>
            <p class="common-body-text">
               We attempted to use <a href="https://en.wikipedia.org/wiki/Convolutional_neural_network#Artificial_data">data augmentation</a> to enhance performance. 
            </p>
            <p class="common-body-text">
               We augmented our data by randomly replacing words with the UNK token.
            </p>
            <p class="common-body-text">
               We tried this with several probabilities (up to 70%).
            </p>
            <p class="common-body-text">
               We saw no noticeable improvement in F1 score across all of our models. 
            </p>
            <p class="common-body-text">
               The most salient change observed was that the convergence of the CNN and dense neural network slowed by about 2-3 epochs, but they both converged to approximately what they normally would without the data augmentation. 
            </p>
            <p class="common-body-text">
               For future work, another possible method we could've explored was replacing words with synonyms (possibly restricting the synonym choices to only those present in the training set rather than all possible synonyms in the English language). 
            </p>
            <h3>F1 Threshold Optimization</h3>
            <p class="common-body-text">
               When deep neural networks are used in practice to classify samples, the continuous output of the network is discretized frequently using a <a href="https://en.wikipedia.org/wiki/Rounding">round function</a>. This simply sets the threshold at <code>0.5</code>.
            </p>
            <p class="common-body-text">
               Inspired by the idea of threshold selection when reviewing how <a href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic">ROC curves</a> are used with <a href="https://en.wikipedia.org/wiki/Logistic_regression">logistic regression models</a>, we realized that using a threshold of <code>0.5</code> is likely sufficient but not globally optimal. 
            </p>
            <p class="common-body-text">
               A model might have a <a href="https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff">bias</a> that it cannot overcome that'll prevent the positive predictions for a label to be exactly 1 or exactly 0 for a negative prediction. Because of this, it is not clear that for a given label the positive predictions and negative predictions would be centered around <code>0.5</code>.
            </p>
            <p class="common-body-text">
               Thus, we experimented with naively optimizing the threshold with which we would discretize our neural network outputs. 
            </p>
            <p class="common-body-text">
               Our naive method was to chose the threshold for each label to be the center point of the mean of the positive predictions for the label and the mean of the negative predictions for the label . 
            </p>
            <p class="common-body-text">
              This led to a testing F1 score increase of about <code>0.025</code> on all of our models. 
            </p>
            <p class="common-body-text">
              This small but noticeable improvement might be worth investigating further in the future by appropriately accounting for outliers in the center point calculation using a <a href="https://en.wikipedia.org/wiki/Standard_deviation">standard deviation</a> threshold. Another option would be to exclude <a href="https://en.wikipedia.org/wiki/False_positives_and_false_negatives">false positives and false negatives</a>.
            </p>
            <h3>Stopword Removal</h3>
            <p class="common-body-text">
               We removed <a href="https://en.wikipedia.org/wiki/Stop_words">stopwords</a> during data pre-processing. 
            </p>
            <p class="common-body-text">
               This increased the testing F1 score by about <code>0.08</code> in our CNN and dense neural network. 
            </p>
            <p class="common-body-text">
               This led to no noticeable improvement in our RNN.
            </p>
            <p class="common-body-text">
	      We used <a href="https://www.nltk.org/">NLTK</a>'s English <a href="https://en.wikipedia.org/wiki/Stop_words">stopword</a> list for <a href="https://en.wikipedia.org/wiki/Stop_words">stopword</a> removal via <code>nltk.corpus.stopwords.words('english')</code>.
            </p>
            <h3>Contraction Expansion</h3>
            <p class="common-body-text">
               We expanded <a href="https://en.wikipedia.org/wiki/Contraction_(grammar)">contractions</a> during data pre-processing. It yielded no noticeable improvement for any of our models. 
            </p>
            <p class="common-body-text">
	      We implemented our own contraction/shorthand expander. The list of contractions and shorthand can be found <a href="https://github.com/paul-tqh-nguyen/reuters_topic_labelling/blob/master/preprocess_data.py#L47">here</a>.
            </p>
            <h3>Number Tokenization</h3>
            <p class="common-body-text">
               We replaced numbers, e.g. <code>"9"</code>, <code>"3/4"</code>, <code>"1.3"</code>, <code>"1,256,412"</code>, with a special <code>NUMBER</code> token. While this drastically decreased our <code>UNK</code> count in the data and the total number of unknown words (which is expected), it yielded no noticeable improvement for any of our models. It did help us reduce the noise when investigating the impact of unknown words.
            </p>
            <h3>Lemmatization</h3>
            <p class="common-body-text">
               We employed <a href="https://en.wikipedia.org/wiki/Lemmatisation">lemmatization</a> during our data pre-processing. It yielded no noticeable improvement for any of our models. 
            </p>
            <p class="common-body-text">
	      We used <a href="https://www.nltk.org/">NLTK</a>'s <a href="https://wordnet.princeton.edu/">WordNet</a> <a href="https://en.wikipedia.org/wiki/Lemmatisation">lemmatizer</a> via <code>nltk.stem.WordNetLemmatizer()</code>.
            </p>
         </div>
         <div id="implementation-details" class="topic-section">
            <center>
               <h2 class="common-UppercaseTitle">
                  <svg class="section-icon">
                     <circle fill="#eeb5f7" cx="33" cy="33" r="33">
                     </circle>
                     <path d="M20 24c0-1.7 1-2.3 2.5-1.3l13 8.6c1.4 1 1.4 2.4 0 3.4l-13 8.6c-1.4 1-2.5.4-2.5-1.3V24" fill="#9d8fff" fill-opacity="0.8" transform="translate(-2.789979447470614e-8 1.1508664243820022e-8) scale(0.9999999996512525)">
                     </path>
                     <path d="M20 24c0-1.7 1-2.3 2.5-1.3l13 8.6c1.4 1 1.4 2.4 0 3.4l-13 8.6c-1.4 1-2.5.4-2.5-1.3V24" fill="#8e80ff" fill-opacity="1.0" transform="translate(12.001415746356617)">
                     </path>
                  </svg>
                  <span>
                  Implementation Details & Code Demonstration
                  </span>
               </h2>
            </center>
            <p class="common-body-text">
	      In this section, we'll go over how to use the code that we've implemented <a href="https://github.com/paul-tqh-nguyen/reuters_topic_labelling">here</a>. Much of this information is specific to our implementation, so this section is only useful for those who might want to experiemtn with our code directly. 
            </p>
            <p class="common-body-text">
	      We intend for our code to be fairly platform independent via our <a href="https://github.com/paul-tqh-nguyen/reuters_topic_labelling/blob/master/environment.yml">environment.yml</a>. If you're having trouble, please let me know. If you're not familiar with managing environments with <a href="https://docs.conda.io/en/latest/">conda</a>, helpful documentation can be found <a href="https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html">here</a>.
            </p>
            <p class="common-body-text">
	      If you have <a href="https://docs.conda.io/en/latest/">conda</a> installed, simply running <code>conda env create ; conda activate reuters</code> from the checkout directory will handle all dependency issues.
            </p>
            <p class="common-body-text">
            </p>
	    <h3>Command-Line Interface</h3>
            <p class="common-body-text">
	      Much of the functionality discussed in this document can be utilized via our CLI.
            <pre>
		<code>
pnguyen@pnguyenmachine:/tmp/reuters_topic_labelling$ ./main.py
usage: main.py [-h] [-preprocess-data] [-train-model] [-hyperparameter-search] [-hyperparameter-search-rnn] [-hyperparameter-search-conv] [-hyperparameter-search-dense]

optional arguments:
  -h, --help                    show this help message and exit
  -preprocess-data              Preprocess the raw SGML files into a CSV.
  -train-model                  Trains &amp; evaluates our model on our dataset. Saves model to ./best-model.pt.
  -hyperparameter-search        Exhaustively performs -train-model over the hyperparameter space. Details of the best performance are tracked in global_best_model_score.json.
  -hyperparameter-search-rnn    Perform the hyperparameter search on our RNN model.
  -hyperparameter-search-conv   Perform the hyperparameter search on our CNN model.
  -hyperparameter-search-dense  Perform the hyperparameter search on our simple feed-forward model.
pnguyen@pnguyenmachine:/tmp/reuters_topic_labelling$ 
		</code>
	    </pre>
            </p>
	    <h3>Data Pre-processing</h3>
            <p class="common-body-text">
	      We used <a href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/">Beautiful Soup</a> to parse the <code>.sgm</code> files. 
            </p>
            <p class="common-body-text">
	      We used <a href="https://docs.python.org/3/library/multiprocessing.html">Python's native multiprocessing library</a> to process each <code>.sgm</code> file independently. 
            </p>
            <p class="common-body-text">
	      We used <a href="https://www.nltk.org/">NLTK</a>'s English <a href="https://en.wikipedia.org/wiki/Stop_words">stopword</a> list for <a href="https://en.wikipedia.org/wiki/Stop_words">stopword</a> removal via <code>nltk.corpus.stopwords.words('english')</code>.
            </p>
            <p class="common-body-text">
	      We implemented our own contraction/shorthand expander. The list of contractions and shorthand can be found <a href="https://github.com/paul-tqh-nguyen/reuters_topic_labelling/blob/master/preprocess_data.py#L47">here</a>.
            </p>
            <p class="common-body-text">
	      We used <a href="https://www.nltk.org/">NLTK</a>'s <a href="https://wordnet.princeton.edu/">WordNet</a> <a href="https://en.wikipedia.org/wiki/Lemmatisation">lemmatizer</a> via <code>nltk.stem.WordNetLemmatizer()</code>.
            </p>
            <p class="common-body-text">
	      One can easily instantiate our data pre-processing vlia our command-line interface:
	      <pre>
		<code>
pnguyen@pnguyenmachine:/tmp/reuters_topic_labelling$ ./main.py -preprocess-data
Parsing .sgm files.
Parsing of .sgm files complete.
Parsing of .sgm files took 45.32427167892456 seconds.

Preprocessing of entire dataset is in ./preprocessed_data/all_extracted_data.csv
./preprocessed_data/all_extracted_data.csv has 19042 rows.
./preprocessed_data/all_extracted_data.csv has 14 columns.

Preprocessing of topics is in ./preprocessed_data/topics_data.csv
./preprocessed_data/topics_data.csv has 10 topics.
./preprocessed_data/topics_data.csv has 8599 rows.
./preprocessed_data/topics_data.csv has 17 columns.

pnguyen@pnguyenmachine:/tmp/reuters_topic_labelling$ 
		</code>
	      </pre>
            </p>
	    <h3>Deep Learning Models</h3>
            <p class="common-body-text">
	      We'll now go over how to use our deep learning models.
            </p>
            <p class="common-body-text">
	      We used <a href="https://pytorch.org/">PyTorch</a> and <a href="https://pytorch.org/text/">TorchText</a> to implement our models.
            </p>
            <p class="common-body-text">
	      We'll just show how to use our CNN for now. Use of our other models will be analogous.
            </p>
            <p class="common-body-text">
	      <pre>
		<code>
&gt;&gt;&gt; from torch.nn.functional import max_pool1d, avg_pool1d, adaptive_avg_pool1d, adaptive_max_pool1d, lp_pool1d
&gt;&gt;&gt; NUMBER_OF_EPOCHS = 100
&gt;&gt;&gt; BATCH_SIZE = 64
&gt;&gt;&gt; MAX_VOCAB_SIZE = 25_000
&gt;&gt;&gt; TRAIN_PORTION, VALIDATION_PORTION, TESTING_PORTION = (0.50, 0.20, 0.3)
&gt;&gt;&gt; PRE_TRAINED_EMBEDDING_SPECIFICATION = 'fasttext.en.300d'
&gt;&gt;&gt; CONVOLUTION_HIDDEN_SIZE = 256
&gt;&gt;&gt; KERNEL_SIZES = [3,4,5,6]
&gt;&gt;&gt; POOLING_METHOD = max_pool1d
&gt;&gt;&gt; DROPOUT_PROBABILITY = 0.5
&gt;&gt;&gt; OUTPUT_DIR = './default_output/'
&gt;&gt;&gt; 
&gt;&gt;&gt; 
&gt;&gt;&gt; 
&gt;&gt;&gt; from models import ConvClassifier
&gt;&gt;&gt; classifier = ConvClassifier(OUTPUT_DIR, NUMBER_OF_EPOCHS, BATCH_SIZE, TRAIN_PORTION, VALIDATION_PORTION, TESTING_PORTION, MAX_VOCAB_SIZE, PRE_TRAINED_EMBEDDING_SPECIFICATION, convolution_hidden_size=CONVOLUTION_HIDDEN_SIZE, kernel_sizes=KERNEL_SIZES, pooling_method=POOLING_METHOD, dropout_probability=DROPOUT_PROBABILITY)
Original dataset size: 4300
Original dataset chi-squared statistic: 6120.89306640625
Final dataset chi-squared statistic: 6120.89306640625
Oversampled dataset size: 4300
Dataset balancing took 2.1816141605377197 seconds.
&gt;&gt;&gt; classifier.train()

Model hyperparameters are:
        number_of_epochs: 100
        batch_size: 64
        max_vocab_size: 25000
        vocab_size: 15652
        pre_trained_embedding_specification: fasttext.en.300d
        output_size: 10
        output_directory: ./default_output/
        convolution_hidden_size: 256
        dropout_probability: 0.5
        kernel_sizes: [3, 4, 5, 6]
        pooling_method: max_pool1d

The model has 6089274 trainable parameters.
This processes's PID is 3275.

Starting training


Epoch 0
Training F1 0.05750538: 100%|██████████████████████████████████████████████████| 68/68 [00:09&lt;00:00,  6.89it/s]
Optimizing F1 Threshold: 100%|██████████████████████████████████████████████████| 68/68 [00:02&lt;00:00, 31.08it/s]
Validation F1 0.33941791: 100%|██████████████████████████████████████████████████| 41/41 [00:01&lt;00:00, 34.28it/s]
	 Train F1: 0.05750538 | Train Recall: 0.04218872 | Train Precision: 0.10586598 | Train Loss: 0.47773770
	  Val. F1: 0.33941791 |  Val. Recall: 0.80459968 |  Val. Precision: 0.26790681 |  Val. Loss: 0.23443730
Optimizing F1 Threshold: 100%|██████████████████████████████████████████████████| 68/68 [00:02&lt;00:00, 30.84it/s]
Testing F1 0.33346784: 100%|██████████████████████████████████████████████████| 27/27 [00:01&lt;00:00, 21.71it/s]
	  Test F1: 0.33346784 |  Test Recall: 0.76290761 |  Test Precision: 0.26209607 |  Test Loss: 0.23624121
Epoch 0 took 17.25825786590576 seconds.

<strong style="font-size: 1.2em;color: red;">Skipping printing for brevity...</strong>

Epoch 16
Training F1 0.49711680: 100%|██████████████████████████████████████████████████| 68/68 [00:07&lt;00:00,  9.40it/s]
Optimizing F1 Threshold: 100%|██████████████████████████████████████████████████| 68/68 [00:02&lt;00:00, 26.92it/s]
Validation F1 0.81630142: 100%|██████████████████████████████████████████████████| 41/41 [00:01&lt;00:00, 30.55it/s]
	 Train F1: 0.49711680 | Train Recall: 0.40129116 | Train Precision: 0.72455534 | Train Loss: 0.35573775
	  Val. F1: 0.81630142 |  Val. Recall: 0.85080909 |  Val. Precision: 0.80316160 |  Val. Loss: 0.05132463
Optimizing F1 Threshold: 100%|██████████████████████████████████████████████████| 68/68 [00:02&lt;00:00, 28.46it/s]
Testing F1 0.75295177: 100%|██████████████████████████████████████████████████| 27/27 [00:01&lt;00:00, 21.24it/s]
	  Test F1: 0.75295177 |  Test Recall: 0.78413611 |  Test Precision: 0.74552983 |  Test Loss: 0.05430432
Epoch 16 took 14.843393564224243 seconds.



Validation is not better than any of the 5 recent epochs, so training is ending early due to apparent convergence.

Optimizing F1 Threshold: 100%|██████████████████████████████████████████████████| 68/68 [00:02&lt;00:00, 28.09it/s]
Testing F1 0.75295177: 100%|██████████████████████████████████████████████████| 27/27 [00:01&lt;00:00, 20.66it/s]
	  Test F1: 0.75295177 |  Test Recall: 0.78413611 |  Test Precision: 0.74552983 |  Test Loss: 0.05430432
&gt;&gt;&gt; 
&gt;&gt;&gt; 
&gt;&gt;&gt; 
&gt;&gt;&gt; classifier.classify_string('''
The Bank of England said it had provided
the money market with 115 mln stg assistance in the morning
session. This compares with the Bank's forecast of a 300 mln
stg shortage in the system today.
    The central bank bought bills outright in band two at
9-13/16 pct comprising 73 mln stg bank bills and 42 mln stg
local authority bills.
 REUTER
''')
{'money-fx', 'interest'}
&gt;&gt;&gt; 
&gt;&gt;&gt; 
&gt;&gt;&gt; 
&gt;&gt;&gt; classifier.classify_string('''
Shr profit two cts vs loss 58 cts
    Net profit 18.2 mln vs loss 23.9 mln
    Revs 355.6 mln vs 308.2 mln
    Nine mths
    Shr loss 81 cts vs loss 5.52 dlrs
    Net profit 10.7 mln vs loss 290.3 mln
    Revs 1.01 billion vs 983.3 mln
    NOTE: Net income per share is after deductions for
mandatory preferred stock dividends and income from the
chemical operations not attributable to common stockholders.
    1987 qtr and nine mths includes gain of eight cts per share
for the partial redemption of series a preferred stock which
will be paid from the net earnings of the chemicals operations.
1986 nine mths includes loss 247.7 mln dlrs from write-down of
petroleum service assets and other restructuring costs.
 Reuter
''')
{'earn'}
&gt;&gt;&gt; 
&gt;&gt;&gt; 
&gt;&gt;&gt; 
&gt;&gt;&gt; classifier.classify_string('''
U.S. farmers who in the past have
grown oats for their own use but failed to certify to the
government that they had done so probably will be allowed to
continue planting that crop and be eligible for corn program
benefits, an aide to Agriculture Secretary Richard Lyng said.
    Currently a farmer, to be eligible for corn program
benefits, must restrict his plantings of other program crops to
the acreage base for that crop.
    Several members of Congress from Iowa have complained that
farmers who inadvertantly failed to certify that they had grown
oats for their own use in the past now are being asked to halt
oats production or lose corn program benefits.
    USDA likely will allow historic oats farmers to plant oats
but not extend the exemption to all farmers, Lyng's aide said.
 Reuter
''')
{'grain', 'corn', 'wheat'}
&gt;&gt;&gt; 
		</code>
	      </pre>
            </p>
            <p class="common-body-text">
	      NB: Testing only happens when a new validation minimum is achieved and at the end of training.
            </p>
            <p class="common-body-text">
	      NB: Training ends when convergence is achieved, i.e. when 5 epochs pass without a new validation minimum.
            </p>
            <p class="common-body-text">
	      If you're interested in using our RNN or dense neural network, consider using our <a href="https://github.com/paul-tqh-nguyen/reuters_topic_labelling/blob/master/models.py"><code>EEAPClassifier</code></a> and <a href="https://github.com/paul-tqh-nguyen/reuters_topic_labelling/blob/master/models.py"><code>DenseClassifier</code></a> classes. Usage will be analogous to what's shown above.
            </p>
            <p class="common-body-text">
            </p>
            <p class="common-body-text">
            </p>
            <p class="common-body-text">
            </p>
            <p class="common-body-text">
            </p>
         </div>
         <div id="conclusion" class="topic-section">
            <center>
               <h2 class="common-UppercaseTitle">
                  <svg class="section-icon">
                     <circle fill="#eeb5f7" cx="33" cy="33" r="33"></circle>
                     <rect fill="none" stroke="#8e80ff" stroke-width="2" x="22" y="22" width="22" height="22" rx="1"></rect>
                     <path d="M26.10164627022045,32.09865305163782 C25.001845818125954,30.998852599543326 24.001995479055086,31.298752825590572 24.001995479055086,32.998503390708684 L24.001995479055086,39.99800452094492 C24.001995479055086,41.09800452094491 24.901995479055085,41.99800452094492 26.001995479055086,41.99800452094492 L33.001496609291316,41.99800452094492 C34.70124717440943,41.99800452094492 35.10119728743305,41.09820406885042 33.90134694836218,39.89835372977955 L31.80139683533856,37.79840361675593 L37.79840361675593,31.80139683533856 L39.89835372977955,33.90134694836218 C40.99815418187404,35.001147400456674 41.99800452094492,34.70124717440943 41.99800452094492,33.001496609291316 L41.99800452094492,26.001995479055086 C41.99800452094492,24.901995479055085 41.09800452094491,24.001995479055086 39.99800452094492,24.001995479055086 L32.998503390708684,24.001995479055086 C31.298752825590572,24.001995479055086 30.898802712566948,24.901795931149575 32.09865305163782,26.10164627022045 L34.298653051637814,28.301646270220445 L28.301646270220445,34.298653051637814 L26.10164627022045,32.09865305163782" stroke="#fde0e0" stroke-width="2" fill="#8e80ff"></path>
                  </svg>
                  <span>
                  Conclusion & Lessons Learned
                  </span>
               </h2>
            </center>
            <div style="opacity:0.05;"><div id="stripes" style="background: darkviolet;"></div></div>
            <p class="common-body-text">
               According to Knuth's definition, a B-tree of order
            </p>
         </div>
      </main>
      <footer class="globalFooter withCards">
         <section class="globalFooterCards" style="position: relative">
            <div style="opacity:0.1;"><div id="stripes" style="background: blue;-webkit-transform: skewY(0deg);transform: skewY(0deg);"></div></div>
            <div class="container-xl">
               <a class="globalFooterCard" href="https://github.com/paul-tqh-nguyen" style="text-decoration: none;">
                  <img src="./front_end/github.svg" height="100" width="100"  style="padding-left: 60px;"/>
                  <center>
                     <p class="common-body-text" style="text-align: center; font-weight: bold;">Interested In My Work?</p>
                     </br>
                     <h2 class="common-UppercaseText">See my projects on GitHub.</h2>
                  </center>
               </a>
               <a class="common-Link globalFooterCard card-connect" href="https://paul-tqh-nguyen.github.io/about/" style="text-decoration: none;">
                  <img src="./front_end/website.svg" height="100" width="100"  style="padding-left: 60px;"/>
                  <center>
                     <p class="common-body-text" style="text-align: center; font-weight: bold;">Want to learn more about me?</p>
                     </br>
                     <h2 class="common-UppercaseText">Visit my website.</h2>
                  </center>
               </a>
            </div>
         </section>
      </footer>
   </body>
</html>
